{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMGN51DE3ZZg"
   },
   "source": [
    "### To do: \n",
    "- build Ball Tree for cosine similarity\n",
    "- implement Bayesian optimisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c1pxoCjT4CbP",
    "outputId": "4f7e429c-1150-4f7f-8d34-da78a60281ff"
   },
   "outputs": [],
   "source": [
    "# Install RDKit. Takes 2-3 minutes\n",
    "# !wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# !chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "# !time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "# !time conda install -q -y -c conda-forge python=3.7\n",
    "# !time conda install -q -y -c conda-forge rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "87OYfp5T4QDS",
    "outputId": "7297a371-2c64-4395-de18-8f8b821bd88e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8Q1rtww36JU"
   },
   "outputs": [],
   "source": [
    "# !cp '/content/gdrive/My Drive/rxn_ebm/USPTO_50k_Schneider/clean_rxn_50k_nomap_noreagent.pickle' '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSKTT4aD3ZZh"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/usr/local/lib/python3.7/site-packages/') \n",
    "# for Colab \n",
    "import os\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.ipython_useSVG=True\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import rdChemReactions\n",
    "from rdkit.Chem import rdqueries # faster than iterating atoms https://sourceforge.net/p/rdkit/mailman/message/34538007/ \n",
    "from rdkit.Chem.rdchem import Atom\n",
    "from rdkit import DataStructs\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re \n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMIpqIVk3ZZs"
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfFuy7g03ZZs"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_activation_function(activation: str) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Gets an activation function module given the name of the activation.\n",
    "    Supports:\n",
    "    * :code:`ReLU`\n",
    "    * :code:`LeakyReLU`\n",
    "    * :code:`PReLU`\n",
    "    * :code:`tanh`\n",
    "    * :code:`SELU`\n",
    "    * :code:`ELU`\n",
    "    :param activation: The name of the activation function.\n",
    "    :return: The activation function module.\n",
    "    \"\"\"\n",
    "    if activation == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'LeakyReLU':\n",
    "        return nn.LeakyReLU(0.1)\n",
    "    elif activation == 'PReLU':\n",
    "        return nn.PReLU()\n",
    "    elif activation == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif activation == 'SELU':\n",
    "        return nn.SELU()\n",
    "    elif activation == 'ELU':\n",
    "        return nn.ELU()\n",
    "    else:\n",
    "        raise ValueError(f'Activation \"{activation}\" not supported.')\n",
    "    \n",
    "def initialize_weights(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Initializes the weights of a model in place.\n",
    "    :param model: An PyTorch model.\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)\n",
    "            \n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1Cfy0AF3ZZv"
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxvJUWjr3ZZw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FF_ebm(nn.Module):\n",
    "    '''\n",
    "    trainargs: dictionary containing hyperparameters to be optimised, \n",
    "    hidden_sizes must be a list e.g. [1024, 512, 256]\n",
    "    \n",
    "    To do: bayesian optimisation\n",
    "    '''\n",
    "    def __init__(self, trainargs):\n",
    "        super(FF_ebm, self).__init__()\n",
    "        self.output_size = trainargs['output_size']\n",
    "        self.num_layers = len(trainargs['hidden_sizes']) + 1\n",
    "\n",
    "        if trainargs['model'] == 'FF_sep':\n",
    "          self.input_dim = trainargs['rctfp_size'] + trainargs['prodfp_size'] # will be rctfp_size + prodfp_size for FF_sep\n",
    "        elif trainargs['model'] == 'FF_diff':\n",
    "          self.input_dim = trainargs['rctfp_size']\n",
    "          assert trainargs['rctfp_size'] == trainargs['prodfp_size'], 'rctfp_size != prodfp_size, unable to make difference FPs!!!'\n",
    "\n",
    "        self.create_ffn(trainargs)\n",
    "        initialize_weights(self)  # is it necessary to initialize weights?? \n",
    "    \n",
    "    def create_ffn(self, trainargs):\n",
    "        '''\n",
    "        Creates feed-forward network using trainargs dict\n",
    "        '''\n",
    "        dropout = nn.Dropout(trainargs['dropout'])\n",
    "        activation = get_activation_function(trainargs['activation'])\n",
    "\n",
    "        if self.num_layers == 1:\n",
    "            ffn = [\n",
    "                dropout,\n",
    "                nn.Linear(self.input_dim, self.output_size)\n",
    "            ]\n",
    "        else:\n",
    "            ffn = [\n",
    "                dropout,\n",
    "                nn.Linear(self.input_dim, trainargs['hidden_sizes'][0])\n",
    "            ]\n",
    "            \n",
    "            # intermediate hidden layers \n",
    "            for i, layer in enumerate(range(self.num_layers - 2)):\n",
    "                ffn.extend([\n",
    "                    activation,\n",
    "                    dropout,\n",
    "                    nn.Linear(trainargs['hidden_sizes'][i], trainargs['hidden_sizes'][i+1]),\n",
    "                ])\n",
    "                \n",
    "            # last hidden layer\n",
    "            ffn.extend([\n",
    "                activation,\n",
    "                dropout,\n",
    "                nn.Linear(trainargs['hidden_sizes'][-1], self.output_size),\n",
    "            ])\n",
    "\n",
    "        self.ffn = nn.Sequential(*ffn)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        '''\n",
    "        Runs FF_ebm on input\n",
    "        \n",
    "        batch: a N x K x 1 tensor of N training samples, where each sample contains \n",
    "        a positive rxn on the first column, and K-1 negative rxn on subsequent columns \n",
    "        supplied by DataLoader on custom ReactionDataset \n",
    "        '''\n",
    "        energy_scores = self.ffn(batch) # tensor of size N x K x 1\n",
    "        return energy_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7EzzESW3ZZz"
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeAWhxLw3ZZz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class Run():\n",
    "    def __init__(self, model, trainargs,\n",
    "                 optimizer=None, load_checkpoint=False):\n",
    "        self.device = trainargs['device']\n",
    "        model = model.to(self.device)\n",
    "        self.model = model\n",
    "        if load_checkpoint: \n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = trainargs['optimizer'](model.parameters(), lr=trainargs['learning_rate'])\n",
    "        self.trainargs = trainargs \n",
    "\n",
    "        train_dataset = ReactionDataset(trainargs['path_to_pickle'], 'train', trainargs)\n",
    "        self.train_loader = DataLoader(train_dataset, trainargs['batch_size'], shuffle=True)\n",
    "        \n",
    "        val_dataset = ReactionDataset(trainargs['path_to_pickle'], 'valid', trainargs)\n",
    "        self.val_loader = DataLoader(val_dataset, 2 * trainargs['batch_size'], shuffle=False)\n",
    "        \n",
    "        test_dataset = ReactionDataset(self.trainargs['path_to_pickle'], 'test', self.trainargs)\n",
    "        self.test_loader = DataLoader(test_dataset, 2 * self.trainargs['batch_size'], shuffle=False)\n",
    "        del train_dataset, val_dataset, test_dataset\n",
    "\n",
    "        self.mean_train_loss = []\n",
    "        self.min_val_loss = 1e9\n",
    "        self.mean_val_loss = []\n",
    "        self.best_epoch = None\n",
    "        self.stats = {'trainargs': self.trainargs} # to store training statistics  \n",
    "\n",
    "        torch.manual_seed(trainargs['model_seed'])\n",
    "        random.seed(trainargs['random_seed'])\n",
    "    \n",
    "    def train_one(self, batch, val=False):\n",
    "        '''\n",
    "        Trains model for 1 epoch \n",
    "\n",
    "        TO DO: learning rate scheduler + logger \n",
    "        '''\n",
    "        self.model.zero_grad()\n",
    "        scores = self.model(batch).squeeze(dim=-1) # scores: size N x K x 1 --> N x K after squeezing\n",
    "\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        probs = softmax(scores) # size N x K\n",
    "\n",
    "        # positives are the 0-th index of each sample, add a small epsilon 1e-9 to stabilise log \n",
    "        loss = -torch.log(probs[:, 0]+1e-9).mean() # probs[:, 0] is size N x 1 --> sum/mean to 1 value\n",
    "\n",
    "        if not val:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "        #     if args.grad_clip:\n",
    "        #         nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return loss.data.cpu()\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        Trains model for num_epochs provided in trainargs\n",
    "        Currently supports feed-forward networks: \n",
    "            FF_diff: takes as input a difference FP of fp_size & fp_radius\n",
    "            FF_sep: takes as input a concatenation of [reactants FP, product FP] \n",
    "\n",
    "        trainargs: dict of params \n",
    "        '''\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in np.arange(self.trainargs['epochs']):\n",
    "            self.model.train() # set model to training mode\n",
    "            train_loss = []\n",
    "            for batch in tqdm(self.train_loader): \n",
    "                batch = batch.to(self.device)\n",
    "                train_loss.append(self.train_one(batch, val=False))\n",
    "                self.mean_train_loss.append(np.mean(train_loss)) \n",
    "\n",
    "            self.model.eval() # validation mode\n",
    "            val_loss = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(self.val_loader):\n",
    "                    batch = batch.to(self.device)\n",
    "                    val_loss.append(self.train_one(batch, val=True))\n",
    "                \n",
    "                self.mean_val_loss.append(np.mean(val_loss))\n",
    "                if self.trainargs['early_stop'] and \\\n",
    "                self.min_val_loss - self.mean_val_loss[-1] < self.trainargs['min_delta']:\n",
    "                    if self.trainargs['patience'] <= wait:\n",
    "                        print('Early stopped at the end of epoch: ', epoch)\n",
    "                        print('mean_val_loss: ', np.mean(val_loss))\n",
    "                        stats['early_stop_epoch'] = epoch \n",
    "                        break \n",
    "                    else:\n",
    "                        wait += 1\n",
    "                        print('Decrease in val loss < min_delta, patience count: ', wait)\n",
    "                else:\n",
    "                    wait = 0\n",
    "                    self.min_val_loss = min(self.min_val_loss, self.mean_val_loss[-1])\n",
    "                \n",
    "                if self.mean_val_loss[-1] < self.min_val_loss:\n",
    "                    self.best_epoch = epoch # track best_epoch to load best_checkpoint \n",
    "\n",
    "            if self.trainargs['checkpoint']: # adapted from moco: main_moco.py\n",
    "                save_checkpoint({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model': self.trainargs['model'],\n",
    "                        'state_dict': self.model.state_dict(),\n",
    "                        'optimizer' : self.optimizer.state_dict(),\n",
    "                        'stats' : self.stats,\n",
    "                    }, is_best=False, \n",
    "                    filename=self.trainargs['checkpoint_path']+'{}_{}_checkpoint_{:04d}.pth.tar'.format(\n",
    "                        self.trainargs['model'], self.trainargs['expt_name'], epoch))\n",
    "\n",
    "            print('Epoch: {}, train_loss: {}, val_loss: {}'.format(epoch, \n",
    "                                             np.around(np.mean(train_loss), decimals=4), \n",
    "                                             np.around(np.mean(val_loss), decimals=4)))\n",
    "\n",
    "        self.stats['mean_train_loss'] = self.mean_train_loss\n",
    "        self.stats['mean_val_loss'] = self.mean_val_loss\n",
    "        self.stats['min_val_loss'] = self.min_val_loss\n",
    "        self.stats['best_epoch'] = self.best_epoch\n",
    "        self.stats['train_time'] = time.time() - start \n",
    "        torch.save(self.stats, self.trainargs['checkpoint_path']+'{}_{}_stats.pkl'.format(\n",
    "            self.trainargs['model'], self.trainargs['expt_name']))         # save training stats\n",
    "\n",
    "    def test(self):\n",
    "        '''\n",
    "        Evaluates the model on the test set \n",
    "        '''\n",
    "        test_loss = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader):\n",
    "                batch = batch.to(self.device)\n",
    "                test_loss.append(self.train_one(batch, val=True))\n",
    "\n",
    "        self.stats['test_loss'] = test_loss \n",
    "        self.stats['mean_test_loss'] = np.mean(test_loss)\n",
    "        print('train_time: {}'.format(self.stats['train_time']))\n",
    "        print('test_loss: {}'.format(self.stats['test_loss']))\n",
    "        print('mean_test_loss: {}'.format(self.stats['mean_test_loss']))\n",
    "        # overrides training stats w/ training + test stats\n",
    "        torch.save(self.stats, self.trainargs['checkpoint_path']+'{}_{}_stats.pkl'.format(\n",
    "            self.trainargs['model'], self.trainargs['expt_name'])) \n",
    "\n",
    "    def get_scores(self, dataloader):\n",
    "        ''' \n",
    "        Gets raw energy values (scores) from a trained model on a given dataloader\n",
    "        '''\n",
    "        scores = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "          for batch in tqdm(dataloader):\n",
    "              batch = batch.to(self.device)\n",
    "              self.model.zero_grad()\n",
    "              scores.append(self.model(batch).squeeze(dim=-1)) \n",
    "            # scores: size N x K x 1 --> N x K after squeezing\n",
    "\n",
    "        return torch.cat(scores, dim=0)\n",
    "\n",
    "    def get_topk_acc(self, dataloader, k=1):\n",
    "        '''\n",
    "        Computes top-k accuracy of trained model in classifying feasible vs infeasible chemical rxns\n",
    "        (i.e. maximum energy score assigned to label 0 of each training sample) \n",
    "        '''\n",
    "        self.model.eval()\n",
    "        scores = self.get_scores(dataloader)\n",
    "        predicted_labels = torch.topk(scores, k, dim=1)[1]\n",
    "        \n",
    "        return torch.where(predicted_labels == 0)[0].shape[0]/predicted_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_o12DGGe3ZZ2"
   },
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYdV62U63ZZ3"
   },
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/tutorials/blob/master/beginner_source/data_loading_tutorial.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdChemReactions\n",
    "from rdkit import DataStructs\n",
    "import numpy as np\n",
    "\n",
    "def create_rxn_MorganFP(rxn_smi, fp_type='diff', radius=2, \n",
    "                        rctfp_size=16384, prodfp_size=16384, \n",
    "                        useChirality=True, dtype='int8'):\n",
    "    '''\n",
    "    fp_type: 'diff' or 'sep', \n",
    "    'diff' (difference):\n",
    "    Creates reaction MorganFP following Schneider et al in J. Chem. Inf. Model. 2015, 55, 1, 39–53\n",
    "    reactionFP = productFP - sum(reactantFPs)\n",
    "    \n",
    "    'sep' (separate):\n",
    "    Creates separate reactantsFP and productFP following Gao et al in ACS Cent. Sci. 2018, 4, 11, 1465–1476\n",
    "    '''\n",
    "    # initialise empty fp numpy arrays\n",
    "    if fp_type == 'diff':\n",
    "        diff_fp = np.empty(rctfp_size, dtype = dtype)\n",
    "    elif fp_type == 'sep':\n",
    "        rcts_fp = np.empty(rctfp_size, dtype = dtype)\n",
    "        prod_fp = np.empty(prodfp_size, dtype = dtype)\n",
    "    else:\n",
    "        print('ERROR: fp_type not recognised!')\n",
    "        return\n",
    "    \n",
    "    # create product FP\n",
    "    prod_mol = Chem.MolFromSmiles(rxn_smi.split('>')[-1])\n",
    "    try:\n",
    "        prod_fp_bit = AllChem.GetMorganFingerprintAsBitVect(\n",
    "                        mol=prod_mol, radius=radius, nBits=prodfp_size, useChirality=useChirality)\n",
    "\n",
    "        fp = np.empty(prodfp_size, dtype = dtype)   # temporarily store numpy array as fp \n",
    "        DataStructs.ConvertToNumpyArray(prod_fp_bit, fp)\n",
    "        if fp_type == 'diff':\n",
    "            diff_fp += fp\n",
    "        elif fp_type == 'sep':\n",
    "            prod_fp = fp\n",
    "    except Exception as e:\n",
    "        print(\"Cannot build product fp due to {}\".format(e))\n",
    "        return\n",
    "                                  \n",
    "    # create reactant FPs, subtracting each from product FP\n",
    "    rcts_smi = rxn_smi.split('>')[0].split('.')\n",
    "    for rct_smi in rcts_smi:\n",
    "        rct_mol = Chem.MolFromSmiles(rct_smi)\n",
    "        try:\n",
    "            rct_fp_bit = AllChem.GetMorganFingerprintAsBitVect(\n",
    "                            mol=rct_mol, radius=radius, nBits=rctfp_size, useChirality=useChirality)\n",
    "            fp = np.empty(rctfp_size, dtype = dtype)\n",
    "            DataStructs.ConvertToNumpyArray(rct_fp_bit, fp)\n",
    "            if fp_type == 'diff':\n",
    "                diff_fp -= fp\n",
    "            elif fp_type == 'sep':\n",
    "                rcts_fp += fp\n",
    "        except Exception as e:\n",
    "            print(\"Cannot build reactant fp due to {}\".format(e))\n",
    "            return\n",
    "    \n",
    "    if fp_type == 'diff':\n",
    "        return diff_fp\n",
    "    elif fp_type == 'sep':\n",
    "        return np.concatenate([rcts_fp, prod_fp])\n",
    "\n",
    "    \n",
    "class ReactionDataset(Dataset):\n",
    "    '''\n",
    "    The Dataset class ReactionDataset prepares training samples of length K: \n",
    "    [pos_rxn, neg_rxn_1, ..., neg_rxn_K-1], ... where K-1 = num_neg \n",
    "\n",
    "    TO DO: can this be further optimised? Augmentation is the clear bottleneck during training\n",
    "    '''\n",
    "    def __init__(self, path_to_pickle, key, trainargs):\n",
    "        '''\n",
    "        pickle is dict w/ keys 'train', 'valid', 'test' each storing a list of rxn_smiles (str)\n",
    "        IMPORTANT: molAtomMapNumbers have been cleared during data pre-processing \n",
    "        ''' \n",
    "        # feels like loading the entire pickle is not feasible when the dataset gets larger \n",
    "        # is there a more memory-efficient way to do this? \n",
    "        with open(path_to_pickle, 'rb') as handle: \n",
    "            self.rxn_smiles = pickle.load(handle)[key] \n",
    "        # \n",
    "        \n",
    "        self.fp_radius = trainargs['fp_radius']\n",
    "        self.fp_type = trainargs['fp_type']\n",
    "        self.rctfp_size = trainargs['rctfp_size']\n",
    "        self.prodfp_size = trainargs['prodfp_size']\n",
    "        self.num_neg = trainargs['num_neg']\n",
    "    \n",
    "    def random_sample_negative(self, pos_rxn_smi, pos_rxn_idx):\n",
    "        '''\n",
    "        Generates 1 negative reaction given a positive reaction SMILES\n",
    "        Returns neg_rxn_smi (str)\n",
    "        '''\n",
    "        rcts_smi = pos_rxn_smi.split('>')[0].split('.')\n",
    "        prod_smi = pos_rxn_smi.split('>')[-1]       \n",
    "            \n",
    "        rct_or_prod = random.choice([0, 1])\n",
    "        if rct_or_prod == 0: # randomly change one of the reactant(s)\n",
    "            orig_idx = random.choice(np.arange(len(rcts_smi))) # randomly choose 1 reactant to be replaced\n",
    "            \n",
    "            found = False\n",
    "            while not found: # searches randomly to find a different rct molecule to swap with \n",
    "                rdm_rxn_idx = random.choice(np.arange(len(self.rxn_smiles))) # randomly choose 1 rxn\n",
    "                if rdm_rxn_idx == pos_rxn_idx: continue # don't choose the original rxn\n",
    "                        \n",
    "                new_rxn_smi = self.rxn_smiles[rdm_rxn_idx]\n",
    "                new_rcts_smi = new_rxn_smi.split('>')[0].split('.')\n",
    "\n",
    "                rdm_rcts_idx = random.choice(np.arange(len(new_rcts_smi)))\n",
    "                if new_rcts_smi[rdm_rcts_idx] != rcts_smi[orig_idx]:\n",
    "                    found = True\n",
    "                    rcts_smi[orig_idx] = new_rcts_smi[rdm_rcts_idx]\n",
    "            \n",
    "        else: # randomly change the product            \n",
    "            found = False\n",
    "            while not found:  # searches randomly to find a different prod molecule to swap with \n",
    "                rdm_rxn_idx = random.choice(np.arange(len(self.rxn_smiles)))\n",
    "                if rdm_rxn_idx == pos_rxn_idx: continue # don't choose the original rxn\n",
    "                        \n",
    "                new_rxn_smi = self.rxn_smiles[rdm_rxn_idx]      \n",
    "                new_prod_smi = new_rxn_smi.split('>')[-1]\n",
    "                if new_prod_smi != prod_smi:\n",
    "                    found = True\n",
    "                    prod_smi = new_prod_smi\n",
    "        \n",
    "        return '{}>>{}'.format('.'.join(rcts_smi), prod_smi)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "      ''' \n",
    "      Returns 1 training sample in the form [pos_rxn, neg_rxn_1, ..., neg_rxn_K-1]\n",
    "      num_neg: a hyperparameter to be tuned\n",
    "      '''\n",
    "      if torch.is_tensor(idx): # may not be needed, taken from data loading tutorial\n",
    "          idx = idx.tolist() \n",
    "\n",
    "      pos_rxn_smi = self.rxn_smiles[idx]\n",
    "      pos_rxn_fp = create_rxn_MorganFP(pos_rxn_smi, radius=self.fp_radius, \n",
    "                                      rctfp_size=self.rctfp_size, prodfp_size=self.prodfp_size, fp_type=self.fp_type)\n",
    "      \n",
    "      assert self.num_neg > 0, 'num_neg cannot be negative!'\n",
    "      neg_rxn_smis = [self.random_sample_negative(pos_rxn_smi, idx) for i in range(self.num_neg)]\n",
    "      neg_rxn_fps = [create_rxn_MorganFP(neg_rxn_smi, radius=self.fp_radius,  \n",
    "                                        rctfp_size=self.rctfp_size, prodfp_size=self.prodfp_size, fp_type=self.fp_type)\n",
    "                    for neg_rxn_smi in neg_rxn_smis]\n",
    "\n",
    "      return torch.Tensor([pos_rxn_fp, *neg_rxn_fps])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rxn_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ti5ndWB3ZaA"
   },
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "890bgQ783ZaB"
   },
   "outputs": [],
   "source": [
    "trainargs = {\n",
    "    'model': 'FF_diff', # must change both model & fp_type \n",
    "    'hidden_sizes': [256],  \n",
    "    'output_size': 1,\n",
    "    'dropout': 0.3, # adapted from Reaction Condition Recommender   \n",
    "    \n",
    "    'batch_size': 512,\n",
    "    'activation': 'ReLU', # trying ELU for its differentiability everywhere (vs ReLU which is not differentiable at x=0)\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'learning_rate': 5e-5, # to try: integrate w/ fast.ai lr_finder & lr_schedulers \n",
    "    'epochs': 50,\n",
    "    'early_stop': True,\n",
    "    'min_delta': 1e-5, \n",
    "    'patience': 3,\n",
    "\n",
    "    'checkpoint': True,\n",
    "    'model_seed': 1337,\n",
    "    'random_seed': 0, # affects neg rxn sampling since it is random\n",
    "    \n",
    "    'rctfp_size': 1024, # if fp_type == 'diff', ensure that both rctfp_size & prodfp_size are identical!\n",
    "    'prodfp_size': 1024,\n",
    "    'fp_radius': 2,\n",
    "    'fp_type': 'diff',\n",
    "    \n",
    "    'num_neg': 1, # to be tuned, 9 seems to be superior to 5 (overfitting occured quickly)\n",
    "    \n",
    "    'path_to_pickle': 'clean_rxn_50k_nomap_noreagent.pickle', \n",
    "    'checkpoint_path': 'checkpoints/',\n",
    "    'expt_name': 'test',\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "9mlGGCOZBEz4",
    "outputId": "93c963dd-f709-4f35-991a-880268233e72"
   },
   "outputs": [],
   "source": [
    "# initialises fingerprint-based feedforward EBM model \n",
    "model = FF_ebm(trainargs)\n",
    "run = Run(model, trainargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oAQVPmtNBAuo",
    "outputId": "d4d3cb35-84d5-4104-ce9b-b67a88db7d06"
   },
   "outputs": [],
   "source": [
    "run.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfmFtR4qBjrf"
   },
   "outputs": [],
   "source": [
    "run.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_o3lhYIioRA"
   },
   "outputs": [],
   "source": [
    "run.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0R5fkcaEBkV5"
   },
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "display.Audio(url=\"https://static.sfdict.com/audio/C07/C0702600.mp3\", autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [
    "twEVWkj63ZZ5"
   ],
   "name": "rad = 3 num_neg = 5 Feedforward_+_diffFP_.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
