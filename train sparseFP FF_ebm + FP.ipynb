{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMGN51DE3ZZg"
   },
   "source": [
    "### To do:  (latest notebook is here)\n",
    "- build Ball Tree for cosine similarity\n",
    "- implement Bayesian optimisation \n",
    "\n",
    "Qns: \n",
    "- are morganFP deterministically calculated --> yes. just be very careful when using np.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c1pxoCjT4CbP",
    "outputId": "4f7e429c-1150-4f7f-8d34-da78a60281ff"
   },
   "outputs": [],
   "source": [
    "# Install RDKit. Takes 2-3 minutes\n",
    "# !wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# !chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "# !time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "# !time conda install -q -y -c conda-forge python=3.7\n",
    "# !time conda install -q -y -c conda-forge rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "87OYfp5T4QDS",
    "outputId": "7297a371-2c64-4395-de18-8f8b821bd88e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8Q1rtww36JU"
   },
   "outputs": [],
   "source": [
    "# !cp '/content/gdrive/My Drive/rxn_ebm/USPTO_50k_Schneider/clean_rxn_50k_nomap_noreagent.pickle' '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = True \n",
    "# change folders as needed\n",
    "if LOCAL: \n",
    "    checkpoint_folder = 'checkpoints/'\n",
    "    base_path = 'USPTO_50k_data/clean_rxn_50k_sparse_FPs_numrcts'\n",
    "else: # colab \n",
    "    checkpoint_folder = '/content/gdrive/My Drive/rxn_ebm/checkpoints/' \n",
    "    base_path = '/content/clean_rxn_50k_sparse_FPs_numrcts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSKTT4aD3ZZh"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/usr/local/lib/python3.7/site-packages/') \n",
    "# for Colab \n",
    "import os\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.ipython_useSVG=True\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import rdChemReactions\n",
    "from rdkit.Chem import rdqueries # faster than iterating atoms https://sourceforge.net/p/rdkit/mailman/message/34538007/ \n",
    "from rdkit.Chem.rdchem import Atom\n",
    "from rdkit import DataStructs\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "getattr(tqdm, '_instances', {}).clear()\n",
    "import csv\n",
    "import re \n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMIpqIVk3ZZs"
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfFuy7g03ZZs"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_activation_function(activation: str) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Gets an activation function module given the name of the activation.\n",
    "    Supports:\n",
    "    * :code:`ReLU`\n",
    "    * :code:`LeakyReLU`\n",
    "    * :code:`PReLU`\n",
    "    * :code:`tanh`\n",
    "    * :code:`SELU`\n",
    "    * :code:`ELU`\n",
    "    :param activation: The name of the activation function.\n",
    "    :return: The activation function module.\n",
    "    \"\"\"\n",
    "    if activation == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'LeakyReLU':\n",
    "        return nn.LeakyReLU(0.1)\n",
    "    elif activation == 'PReLU':\n",
    "        return nn.PReLU()\n",
    "    elif activation == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif activation == 'SELU':\n",
    "        return nn.SELU()\n",
    "    elif activation == 'ELU':\n",
    "        return nn.ELU()\n",
    "    else:\n",
    "        raise ValueError(f'Activation \"{activation}\" not supported.')\n",
    "    \n",
    "def initialize_weights(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Initializes the weights of a model in place.\n",
    "    :param model: An PyTorch model.\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)\n",
    "            \n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1Cfy0AF3ZZv"
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxvJUWjr3ZZw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FF_ebm(nn.Module):\n",
    "    '''\n",
    "    trainargs: dictionary containing hyperparameters to be optimised, \n",
    "    hidden_sizes must be a list e.g. [1024, 512, 256]\n",
    "    \n",
    "    To do: bayesian optimisation\n",
    "    '''\n",
    "    def __init__(self, trainargs):\n",
    "        super(FF_ebm, self).__init__()\n",
    "        self.output_size = trainargs['output_size']\n",
    "        self.num_layers = len(trainargs['hidden_sizes']) + 1\n",
    "\n",
    "        if trainargs['model'] == 'FF_sep':\n",
    "          self.input_dim = trainargs['rctfp_size'] + trainargs['prodfp_size'] # will be rctfp_size + prodfp_size for FF_sep\n",
    "        elif trainargs['model'] == 'FF_diff':\n",
    "          self.input_dim = trainargs['rctfp_size']\n",
    "          assert trainargs['rctfp_size'] == trainargs['prodfp_size'], 'rctfp_size != prodfp_size, unable to make difference FPs!!!'\n",
    "\n",
    "        self.create_ffn(trainargs)\n",
    "        initialize_weights(self)  # is it necessary to initialize weights?? \n",
    "    \n",
    "    def create_ffn(self, trainargs):\n",
    "        '''\n",
    "        Creates feed-forward network using trainargs dict\n",
    "        '''\n",
    "        dropout = nn.Dropout(trainargs['dropout'])\n",
    "        activation = get_activation_function(trainargs['activation'])\n",
    "\n",
    "        if self.num_layers == 1:\n",
    "            ffn = [\n",
    "                dropout,\n",
    "                nn.Linear(self.input_dim, self.output_size)\n",
    "            ]\n",
    "        else:\n",
    "            ffn = [\n",
    "                dropout,\n",
    "                nn.Linear(self.input_dim, trainargs['hidden_sizes'][0])\n",
    "            ]\n",
    "            \n",
    "            # intermediate hidden layers \n",
    "            for i, layer in enumerate(range(self.num_layers - 2)):\n",
    "                ffn.extend([\n",
    "                    activation,\n",
    "                    dropout,\n",
    "                    nn.Linear(trainargs['hidden_sizes'][i], trainargs['hidden_sizes'][i+1]),\n",
    "                ])\n",
    "                \n",
    "            # last hidden layer\n",
    "            ffn.extend([\n",
    "                activation,\n",
    "                dropout,\n",
    "                nn.Linear(trainargs['hidden_sizes'][-1], self.output_size),\n",
    "            ])\n",
    "\n",
    "        self.ffn = nn.Sequential(*ffn)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        '''\n",
    "        Runs FF_ebm on input\n",
    "        \n",
    "        batch: a N x K x 1 tensor of N training samples, where each sample contains \n",
    "        a positive rxn on the first column, and K-1 negative rxn on subsequent columns \n",
    "        supplied by DataLoader on custom ReactionDataset \n",
    "        '''\n",
    "        energy_scores = self.ffn(batch) # tensor of size N x K x 1\n",
    "        return energy_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7EzzESW3ZZz"
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeAWhxLw3ZZz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class Run():\n",
    "    '''\n",
    "    IMPORTANT: epochs are 1-indexed\n",
    "    if load_checkpoint == True, load_optimizer, load_stats & begin_epoch must be provided \n",
    "    '''\n",
    "    def __init__(self, model, trainargs,\n",
    "                 load_optimizer=None, load_checkpoint=False, load_stats=None, begin_epoch=None):\n",
    "        self.device = trainargs['device']\n",
    "        model = model.to(self.device)\n",
    "        self.model = model\n",
    "        self.trainargs = trainargs \n",
    "        self.best_epoch = None # will be automatically assigned after 1 epoch\n",
    "        \n",
    "        if load_checkpoint: \n",
    "            assert load_optimizer is not None, 'load_checkpoint requires load_optimizer!'\n",
    "            self.optimizer = load_optimizer # load optimizer w/ state dict from checkpoint\n",
    "            \n",
    "            assert load_stats is not None, 'load_checkpoint requires load_stats!'\n",
    "            self.stats = load_stats\n",
    "            self.mean_train_loss = self.stats['mean_train_loss']\n",
    "            self.min_val_loss = self.stats['min_val_loss']\n",
    "            self.mean_val_loss = self.stats['mean_val_loss']\n",
    "            \n",
    "            assert begin_epoch is not None, 'load_checkpoint requires begin_epoch!'\n",
    "            self.begin_epoch = begin_epoch\n",
    "\n",
    "        else: # init fresh optimizer \n",
    "            self.optimizer = trainargs['optimizer'](model.parameters(), lr=trainargs['learning_rate'])\n",
    "            \n",
    "            self.mean_train_loss = []\n",
    "            self.min_val_loss = 1e9\n",
    "            self.mean_val_loss = []\n",
    "            self.begin_epoch = 1\n",
    "            self.stats = {'trainargs': self.trainargs, 'train_time': 0} # to store training statistics  \n",
    "\n",
    "        train_dataset = ReactionDataset(trainargs['base_path'], 'train', trainargs)\n",
    "        self.train_loader = DataLoader(train_dataset, trainargs['batch_size'], shuffle=True)\n",
    "        \n",
    "        val_dataset = ReactionDataset(trainargs['base_path'], 'valid', trainargs)\n",
    "        self.val_loader = DataLoader(val_dataset, 2 * trainargs['batch_size'], shuffle=False)\n",
    "        \n",
    "        test_dataset = ReactionDataset(self.trainargs['base_path'], 'test', self.trainargs)\n",
    "        self.test_loader = DataLoader(test_dataset, 2 * self.trainargs['batch_size'], shuffle=False)\n",
    "        del train_dataset, val_dataset, test_dataset # save memory\n",
    "\n",
    "        torch.manual_seed(trainargs['model_seed'])\n",
    "        random.seed(trainargs['random_seed'])\n",
    "    \n",
    "    def train_one(self, batch, val=False):\n",
    "        '''\n",
    "        Trains model for 1 epoch\n",
    "        TO DO: learning rate scheduler + logger \n",
    "        '''\n",
    "        self.model.zero_grad()\n",
    "        scores = self.model(batch).squeeze(dim=-1) # scores: size N x K x 1 --> N x K after squeezing\n",
    "\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        probs = softmax(scores) # size N x K\n",
    "\n",
    "        # positives are the 0-th index of each sample, add a small epsilon 1e-9 to stabilise log \n",
    "        loss = -torch.log(probs[:, 0]+1e-9).mean() # probs[:, 0] is size N x 1 --> sum/mean to 1 value\n",
    "\n",
    "        if not val:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "        #     if args.grad_clip:\n",
    "        #         nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return loss.data.cpu()\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        Trains model for epochs provided in trainargs\n",
    "        Currently supports feed-forward networks: \n",
    "            FF_diff: takes as input a difference FP of fp_size & fp_radius\n",
    "            FF_sep: takes as input a concatenation of [reactants FP, product FP] \n",
    "        '''\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in np.arange(self.begin_epoch, self.trainargs['epochs']): # epochs are 1-indexed (as of 27th Aug 2 am)\n",
    "            self.model.train() # set model to training mode\n",
    "            train_loss = []\n",
    "            for batch in tqdm(self.train_loader): \n",
    "                batch = batch.to(self.device)\n",
    "                train_loss.append(self.train_one(batch, val=False))\n",
    "                self.mean_train_loss.append(np.mean(train_loss)) \n",
    "\n",
    "            self.model.eval() # validation mode\n",
    "            val_loss = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(self.val_loader):\n",
    "                    batch = batch.to(self.device)\n",
    "                    val_loss.append(self.train_one(batch, val=True))\n",
    "                \n",
    "                self.mean_val_loss.append(np.mean(val_loss))\n",
    "                if self.trainargs['early_stop'] and \\\n",
    "                self.min_val_loss - self.mean_val_loss[-1] < self.trainargs['min_delta']:\n",
    "                    if self.trainargs['patience'] <= wait:\n",
    "                        print('Early stopped at the end of epoch: ', epoch)\n",
    "                        print('mean_val_loss: ', np.mean(val_loss))\n",
    "                        stats['early_stop_epoch'] = epoch \n",
    "                        break \n",
    "                    else:\n",
    "                        wait += 1\n",
    "                        print('Decrease in val loss < min_delta, patience count: ', wait)\n",
    "                else:\n",
    "                    wait = 0\n",
    "                    self.min_val_loss = min(self.min_val_loss, self.mean_val_loss[-1])\n",
    "                \n",
    "                if self.mean_val_loss[-1] < self.min_val_loss:\n",
    "                    self.best_epoch = epoch # track best_epoch to load best_checkpoint \n",
    "\n",
    "            if self.trainargs['checkpoint']: # adapted from moco: main_moco.py\n",
    "                save_checkpoint({\n",
    "                        'epoch': epoch, # epochs are 1-indexed\n",
    "                        'model': self.trainargs['model'],\n",
    "                        'state_dict': self.model.state_dict(),\n",
    "                        'optimizer' : self.optimizer.state_dict(),\n",
    "                        'stats' : self.stats,\n",
    "                    }, is_best=False, \n",
    "                    filename=self.trainargs['checkpoint_path']+'{}_{}_checkpoint_{:04d}.pth.tar'.format(\n",
    "                        self.trainargs['model'], self.trainargs['expt_name'], epoch))\n",
    "\n",
    "            print('Epoch: {}, train_loss: {}, val_loss: {}'.format(epoch, \n",
    "                                             np.around(np.mean(train_loss), decimals=4), \n",
    "                                             np.around(np.mean(val_loss), decimals=4)))\n",
    "\n",
    "        self.stats['mean_train_loss'] = self.mean_train_loss\n",
    "        self.stats['mean_val_loss'] = self.mean_val_loss\n",
    "        self.stats['min_val_loss'] = self.min_val_loss\n",
    "        self.stats['best_epoch'] = self.best_epoch\n",
    "        self.stats['train_time'] += (time.time() - start) / 60\n",
    "        torch.save(self.stats, self.trainargs['checkpoint_path']+'{}_{}_stats.pkl'.format(\n",
    "            self.trainargs['model'], self.trainargs['expt_name']))         # save training stats\n",
    "\n",
    "    def test(self, load_stats=None):\n",
    "        '''\n",
    "        Evaluates the model on the test set\n",
    "        '''\n",
    "        test_loss = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.test_loader):\n",
    "                batch = batch.to(self.device)\n",
    "                test_loss.append(self.train_one(batch, val=True))\n",
    "        \n",
    "        if load_stats is not None: \n",
    "            self.stats = load_stats \n",
    "        assert len(self.stats.keys()) > 1, 'You need to provide load_stats!'\n",
    "        \n",
    "        self.stats['test_loss'] = test_loss \n",
    "        self.stats['mean_test_loss'] = np.mean(test_loss)\n",
    "        print('train_time: {}'.format(self.stats['train_time']))\n",
    "        print('test_loss: {}'.format(self.stats['test_loss']))\n",
    "        print('mean_test_loss: {}'.format(self.stats['mean_test_loss']))\n",
    "\n",
    "        # overrides training stats w/ training + test stats\n",
    "        torch.save(self.stats, self.trainargs['checkpoint_path']+'{}_{}_stats.pkl'.format(\n",
    "            self.trainargs['model'], self.trainargs['expt_name']))\n",
    "\n",
    "    def get_scores(self, dataloader, save_neg=False):\n",
    "        ''' \n",
    "        Gets raw energy values (scores) from a trained model on a given dataloader,\n",
    "        with the option to save pos_neg_smis to analyse model performance\n",
    "        '''\n",
    "        scores = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if save_neg:      # save neg rxn smis to analyse model performance           \n",
    "                pos_neg_smis = []\n",
    "                for pos_neg_smi, batch in tqdm(dataloader):\n",
    "                    batch = batch.to(self.device)\n",
    "                    scores.append(self.model(batch).squeeze(dim=-1)) # scores: size N x K x 1 --> N x K after squeezing\n",
    "                    pos_neg_smis.append(pos_neg_smi)\n",
    "                torch.save(pos_neg_smis, self.trainargs['checkpoint_path']+'{}_{}_posnegsmi.pkl'.format(\n",
    "                        self.trainargs['model'], self.trainargs['expt_name']))\n",
    "                \n",
    "                return torch.cat(scores, dim=0), pos_neg_smis\n",
    "            else:\n",
    "                for batch in tqdm(dataloader):\n",
    "                    batch = batch.to(self.device)\n",
    "                    scores.append(self.model(batch).squeeze(dim=-1))\n",
    "                \n",
    "                return torch.cat(scores, dim=0)\n",
    "\n",
    "    def get_topk_acc(self, dataloader, k=1, repeats=1):\n",
    "        '''\n",
    "        Computes top-k accuracy of trained model in classifying feasible vs infeasible chemical rxns\n",
    "        (i.e. maximum score assigned to label 0 of each training sample) \n",
    "        Returns: (list of accs, mean acc, variance of acc)\n",
    "        '''\n",
    "        accs = np.array([])\n",
    "        for repeat in range(repeats):\n",
    "            scores = self.get_scores(dataloader)\n",
    "            predicted_labels = torch.topk(scores, k, dim=1)[1]\n",
    "            accs = np.append(accs, torch.where(predicted_labels == 0)[0].shape[0] / predicted_labels.shape[0])\n",
    "\n",
    "        return accs, accs.mean(), accs.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_o12DGGe3ZZ2"
   },
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/tutorials/blob/master/beginner_source/data_loading_tutorial.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import pickle\n",
    "from scipy import sparse \n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdChemReactions\n",
    "from rdkit import DataStructs\n",
    "import numpy as np\n",
    "\n",
    "def create_rxn_MorganFP_fromFP(raw_fp, num_rcts, fp_type='diff', \n",
    "                               rctfp_size=4096, prodfp_size=4096, dtype='int8'):\n",
    "    '''\n",
    "    fp_type: 'diff' or 'sep', \n",
    "    'diff' (difference):\n",
    "    Creates reaction MorganFP following Schneider et al in J. Chem. Inf. Model. 2015, 55, 1, 39–53\n",
    "    reactionFP = productFP - sum(reactantFPs)\n",
    "    \n",
    "    'sep' (separate):\n",
    "    Creates separate reactantsFP and productFP following Gao et al in ACS Cent. Sci. 2018, 4, 11, 1465–1476\n",
    "    '''\n",
    "    # initialise empty fp numpy arrays\n",
    "    if fp_type == 'diff':\n",
    "        diff_fp = np.zeros(rctfp_size, dtype = dtype)\n",
    "    elif fp_type == 'sep':\n",
    "        rcts_fp = np.zeros(rctfp_size, dtype = dtype)\n",
    "        prod_fp = np.zeros(prodfp_size, dtype = dtype)\n",
    "    else:\n",
    "        print('ERROR: fp_type not recognised!')\n",
    "        return\n",
    "    \n",
    "    # create product FP\n",
    "    try:\n",
    "        fp = raw_fp[-1, :]\n",
    "        if fp_type == 'diff':\n",
    "            diff_fp += fp\n",
    "        elif fp_type == 'sep':\n",
    "            prod_fp = fp\n",
    "    except Exception as e:\n",
    "        print(\"Cannot build product fp due to {}\".format(e))\n",
    "        return\n",
    "                                  \n",
    "    # create reactant FPs, subtracting each from product FP\n",
    "    for i in range(num_rcts):\n",
    "        try:\n",
    "            fp = raw_fp[i, :]\n",
    "            if fp_type == 'diff':\n",
    "                diff_fp -= fp\n",
    "            elif fp_type == 'sep':\n",
    "                rcts_fp += fp\n",
    "        except Exception as e:\n",
    "            print(\"Cannot build reactant fp due to {}\".format(e))\n",
    "            return\n",
    "    \n",
    "    if fp_type == 'diff':\n",
    "        return diff_fp\n",
    "    elif fp_type == 'sep':\n",
    "        return np.concatenate([rcts_fp, prod_fp])\n",
    "    \n",
    "    \n",
    "class ReactionDataset(Dataset):\n",
    "    '''\n",
    "    The Dataset class ReactionDataset prepares training samples of length K: \n",
    "    [pos_rxn, neg_rxn_1, ..., neg_rxn_K-1], ... where K-1 = num_neg \n",
    "\n",
    "    TO DO: can this be further optimised? Augmentation is the clear bottleneck during training\n",
    "    '''\n",
    "    def __init__(self, base_path, key, trainargs, save_neg=False):\n",
    "        '''\n",
    "        base_path is of the form: 'USPTO_50k_data/clean_rxn_50k_sparse_FPs', and according to key parameter,\n",
    "        the correct full path will be used e.g. 'USPTO_50k_data/clean_rxn_50k_sparse_FPs_train.npz'\n",
    "        ''' \n",
    "        self.fp_raw_num_rcts = sparse.load_npz(base_path + '_' + key + '.npz')  \n",
    "        self.fp_type = trainargs['fp_type']\n",
    "        \n",
    "        self.fp_radius = trainargs['fp_radius'] # not needed if loading pre-computed fingerprints\n",
    "        self.rctfp_size = trainargs['rctfp_size']\n",
    "        self.prodfp_size = trainargs['prodfp_size']\n",
    "        assert trainargs['rctfp_size'] == trainargs['prodfp_size']\n",
    "        \n",
    "        self.num_neg = trainargs['num_neg']\n",
    "        self.save_neg = save_neg\n",
    "\n",
    "    def random_sample_negative(self, raw_fp, raw_fp_idx, num_rcts):\n",
    "        '''\n",
    "        Randomly generates 1 negative rxn given a positive rxn fingerprint\n",
    "        Returns neg_rxn_fp (fingerprint)\n",
    "        ''' \n",
    "        rdm_rxn_idx = random.choice(np.arange(self.fp_raw_num_rcts.shape[0])) \n",
    "        new_fp_raw_num_rcts = self.fp_raw_num_rcts[rdm_rxn_idx].toarray()\n",
    "        new_raw_fp, _ = np.split(new_fp_raw_num_rcts, [new_fp_raw_num_rcts.shape[-1]-1], axis=1)\n",
    "        new_raw_fp = new_raw_fp.reshape(-1, self.rctfp_size) #.astype('int8')\n",
    "        \n",
    "        rct_or_prod = random.choice([0, 1])\n",
    "        if rct_or_prod == 0: # randomly change one of the reactant(s)\n",
    "            orig_idx = random.choice(np.arange(num_rcts)) # randomly choose 1 reactant to be replaced\n",
    "            raw_fp[orig_idx, :] = new_raw_fp[orig_idx, :]\n",
    "        else:  # randomly change product \n",
    "            raw_fp[-1, :] = new_raw_fp[-1, :]\n",
    "        return raw_fp \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' \n",
    "        Returns 1 training sample in the form [pos_rxn_fp, neg_rxn_1_fp, ..., neg_rxn_K-1_fp]\n",
    "        num_neg: a hyperparameter to be tuned\n",
    "        '''\n",
    "        if torch.is_tensor(idx): # may not be needed \n",
    "            idx = idx.tolist() \n",
    "\n",
    "        fp_raw_num_rcts = self.fp_raw_num_rcts[idx].toarray() \n",
    "        pos_raw_fp, num_rcts = np.split(fp_raw_num_rcts, [fp_raw_num_rcts.shape[-1]-1], axis=1)\n",
    "        pos_raw_fp = pos_raw_fp.reshape(-1, self.rctfp_size) #.astype('int8')\n",
    "        num_rcts = num_rcts[0][0]\n",
    "        pos_rxn_fp = create_rxn_MorganFP_fromFP(pos_raw_fp, num_rcts, fp_type=self.fp_type, \n",
    "                                                rctfp_size=self.rctfp_size, prodfp_size=self.prodfp_size)\n",
    "\n",
    "        assert self.num_neg > 0, 'num_neg cannot be negative!'\n",
    "        neg_raw_fps = [self.random_sample_negative(pos_raw_fp, idx, num_rcts) for i in range(self.num_neg)]\n",
    "        neg_rxn_fps = [create_rxn_MorganFP_fromFP(neg_raw_fp, num_rcts, fp_type=self.fp_type, \n",
    "                                                  rctfp_size=self.rctfp_size, prodfp_size=self.prodfp_size)\n",
    "                        for neg_raw_fp in neg_raw_fps]\n",
    "        return torch.Tensor([pos_rxn_fp, *neg_rxn_fps])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.fp_raw_num_rcts.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ti5ndWB3ZaA"
   },
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "890bgQ783ZaB"
   },
   "outputs": [],
   "source": [
    "trainargs = {\n",
    "    'model': 'FF_diff', # must change both model & fp_type \n",
    "    'hidden_sizes': [256],  \n",
    "    'output_size': 1,\n",
    "    'dropout': 0.3,  \n",
    "    \n",
    "    'batch_size': 512,\n",
    "    'activation': 'ReLU',  \n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'learning_rate': 5e-5, # to try: lr_finder & lr_schedulers \n",
    "    'epochs': 50,\n",
    "    'early_stop': True,\n",
    "    'min_delta': 1e-5, \n",
    "    'patience': 3,\n",
    "\n",
    "    'checkpoint': True,\n",
    "    'model_seed': 1337,\n",
    "    'random_seed': 0, # affects neg rxn sampling since it is random\n",
    "    \n",
    "    'rctfp_size': 4096, # if fp_type == 'diff', ensure that both rctfp_size & prodfp_size are identical!\n",
    "    'prodfp_size': 4096,\n",
    "    'fp_radius': 3,\n",
    "    'fp_type': 'diff',\n",
    "    \n",
    "    'num_neg': 5, \n",
    "    \n",
    "    'base_path': base_path, # refer to top of notebook \n",
    "    'checkpoint_path': checkpoint_folder,\n",
    "    'expt_name': 'test',\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ReactionDataset('USPTO_50k_data/clean_rxn_50k_sparse_FPs', 'train', trainargs, save_neg=False)\n",
    "train_loader = DataLoader(train_dataset, trainargs['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# old: approx 3 min \n",
    "# new (less restrictive random replacements): approx 3 min too, no significant speed up\n",
    "for i in range(2):\n",
    "    for batch in tqdm(train_loader):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "9mlGGCOZBEz4",
    "outputId": "93c963dd-f709-4f35-991a-880268233e72"
   },
   "outputs": [],
   "source": [
    "# init fingerprint-based feedforward EBM model \n",
    "model = FF_ebm(trainargs)\n",
    "run = Run(model, trainargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oAQVPmtNBAuo",
    "outputId": "d4d3cb35-84d5-4104-ce9b-b67a88db7d06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [03:35<00:00,  2.73s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.00s/it]\n",
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss: 1.722599983215332, val_loss: 1.654099941253662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [04:06<00:00,  3.12s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:31<00:00,  6.25s/it]\n",
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train_loss: 1.6009000539779663, val_loss: 1.524899959564209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [03:19<00:00,  2.53s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.88s/it]\n",
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, train_loss: 1.4660999774932861, val_loss: 1.3840999603271484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [03:29<00:00,  2.65s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.82s/it]\n",
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, train_loss: 1.312600016593933, val_loss: 1.2144999504089355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [03:16<00:00,  2.48s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.66s/it]\n",
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train_loss: 1.1475000381469727, val_loss: 1.0497000217437744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [03:18<00:00,  2.52s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.77s/it]\n",
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, train_loss: 0.9905999898910522, val_loss: 0.8974999785423279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [03:48<00:00,  2.89s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:23<00:00,  4.64s/it]\n",
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, train_loss: 0.8529999852180481, val_loss: 0.7716000080108643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [03:52<00:00,  2.94s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:23<00:00,  4.77s/it]\n",
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, train_loss: 0.7343999743461609, val_loss: 0.6671000123023987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [04:05<00:00,  3.11s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.16s/it]\n",
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, train_loss: 0.6310999989509583, val_loss: 0.5583000183105469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████▌                                                                   | 14/79 [00:39<03:09,  2.91s/it]"
     ]
    }
   ],
   "source": [
    "# training takes ~3 min 50 secs on CPU\n",
    "# ~2 min 7 secs on GPU (colab)\n",
    "run.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfmFtR4qBjrf"
   },
   "outputs": [],
   "source": [
    "run.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_o3lhYIioRA"
   },
   "outputs": [],
   "source": [
    "run.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0R5fkcaEBkV5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://static.sfdict.com/audio/C07/C0702600.mp3\" type=\"audio/mpeg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as display\n",
    "display.Audio(url=\"https://static.sfdict.com/audio/C07/C0702600.mp3\", autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load checkpoint and resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FF_diff_DIFF_4096_1layer_5neg_rad2_ReLU_checkpoint_0017.pth.tar',\n",
       " 'FF_diff_DIFF_4096_1layer_5neg_rad2_ReLU_stats.pkl',\n",
       " 'FF_diff_DIFF_4096_1layer_5neg_rad3_ReLU_checkpoint_0048.pth.tar',\n",
       " 'FF_diff_DIFF_4096_1layer_5neg_rad3_ReLU_checkpoint_0049.pth.tar',\n",
       " 'FF_diff_DIFF_4096_1layer_5neg_rad3_ReLU_stats.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change folders as needed\n",
    "if LOCAL: \n",
    "    checkpoint_folder = 'checkpoints/'\n",
    "    base_path = 'USPTO_50k_data/clean_rxn_50k_sparse_FPs_numrcts'\n",
    "else: # colab \n",
    "    checkpoint_folder = '/content/gdrive/My Drive/rxn_ebm/checkpoints/' \n",
    "    base_path = '/content/clean_rxn_50k_sparse_FPs_numrcts'\n",
    "\n",
    "filenames = [filename for filename in os.listdir(checkpoint_folder) \n",
    "             if '4096' in filename] # narrow down list \n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = 'Adam' # needed to fix bug in name of optimizer when saving checkpoint\n",
    "stats_filename = 'FF_diff_DIFF_4096_1layer_5neg_rad3_ReLU_stats.pkl' # copy & paste from list above \n",
    "\n",
    "\n",
    "curr_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "stats = torch.load(checkpoint_folder + stats_filename, \n",
    "          map_location=torch.device('cpu'))\n",
    "stats['trainargs']['path_to_pickle'] = path_to_pickle\n",
    "stats['trainargs']['checkpoint_path'] = checkpoint_folder\n",
    "\n",
    "if opt == 'Adam':\n",
    "    stats['trainargs']['optimizer'] = torch.optim.Adam # fix bug in name of optimizer when saving checkpoint\n",
    "\n",
    "stats['best_epoch'] = stats['mean_val_loss'].index(stats['min_val_loss'])  # 0-index \n",
    "stats['trainargs']['device'] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "try: \n",
    "    checkpoint_filename = stats_filename[:-9]+'checkpoint_{}.pth.tar'.format(str(stats['best_epoch']).zfill(4)) \n",
    "    checkpoint = torch.load(checkpoint_folder + checkpoint_filename,\n",
    "          map_location=torch.device(curr_device))\n",
    "\n",
    "    model = FF_ebm(stats['trainargs'])\n",
    "    optimizer = stats['trainargs']['optimizer'](model.parameters(), lr=stats['trainargs']['learning_rate'])\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    if torch.cuda.is_available(): # move optimizer tensors to gpu  https://github.com/pytorch/pytorch/issues/2830\n",
    "      for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                state[k] = v.cuda()\n",
    "except:\n",
    "    print('best_epoch checkpoint not found in directory!!!')\n",
    "    print('best_epoch: {}'.format(stats['best_epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainargs = {'activation': 'ReLU',\n",
    " 'batch_size': 512,\n",
    " 'checkpoint': True,\n",
    " 'checkpoint_path': checkpoint_folder,\n",
    " 'device': curr_device,\n",
    " 'dropout': 0.5,\n",
    " 'early_stop': True,\n",
    " 'epochs': 50,\n",
    " 'expt_name': 'cont_DIFF_4096_1layer_5neg_rad3_ReLU',\n",
    " 'fp_radius': 3,\n",
    " 'fp_type': 'diff',\n",
    " 'hidden_sizes': [256],\n",
    " 'learning_rate': 5e-05,\n",
    " 'min_delta': 1e-05,\n",
    " 'model': 'FF_diff',\n",
    " 'model_seed': 1337,\n",
    " 'num_neg': 5,\n",
    " 'optimizer': torch.optim.Adam,\n",
    " 'output_size': 1,\n",
    " 'path_to_pickle': base_path, \n",
    " 'patience': 5,\n",
    " 'prodfp_size': 4096,\n",
    " 'random_seed': 0,\n",
    " 'rctfp_size': 4096}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Run(model, trainargs, optimizer, load_checkpoint=True, load_stats=stats)\n",
    "# run.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "display.Audio(url=\"https://static.sfdict.com/audio/C07/C0702600.mp3\", autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [
    "twEVWkj63ZZ5"
   ],
   "name": "rad = 3 num_neg = 5 Feedforward_+_diffFP_.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
