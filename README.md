# rxnebm
Energy-based modeling of chemical reactions

## Environmental setup
### Using conda
    # ensure conda is already initialized
    bash -i setup.sh
    conda activate rxnebm

## Data preparation
### Download pre-cleaned data (TODO: include --get_graphs/--get_fps option to forcibly download)
    python download_data.py

The downloaded data is automatically checked and will throw an error if any required file is missing.
See Appendix A for a full description of data preprocessing.
**Note** we do not automatically download fingerprints and graph features as these files are much larger. Again, see Appendix A for how to download them manually, or generate them yourself.
 
## Training
Before training, ensure you have 1) the 3 CSV files 2) the 3 precomputed reaction data files (be it fingerprints, rxn_smi, graphs etc.). Refer to Appendix A below for how we generate the reaction data files for a proposer. If you are reloading a trained checkpoint for whatever reason, you additionally need to provide ```--old_expt_name <name>```, ```--date_trained <DD_MM_YYYY>``` and ```--load_checkpoint```. <br><br>
For FF-EBM

    sh scripts/<proposer>/FeedforwardEBM.sh
For Graph-EBM

    sh scripts/<proposer>/GraphEBM.sh
For Transformer-EBM (note that this yields poor results and we only report results on RetroSim)

    sh scripts/<proposer>/TransformerEBM.sh

## Appendix A - Details of data preparation
### Cleaner USPTO-50K dataset
The data was obtained from [the dropbox folder](https://www.dropbox.com/sh/6ideflxcakrak10/AADN-TNZnuGjvwZYiLk7zvwra/schneider50k?dl=0&subfolder_nav_tracking=1) provided by the authors of [GLN](https://github.com/Hanjun-Dai/GLN). 
We rename these 3 csv files from ```raw_{phase}.csv``` to ```'schneider50k_train.csv'```, ```'schneider50k_test.csv'``` and ```'schneider50k_valid.csv'```, and save them to ```rxnebm/data/original_data``` <br>

For the re-ranking task, we trained four popular & state-of-the-art retrosynthesis models. We use a single, extra-clean USPTO_50k dataset, split roughly into 80/10/10. These are derived from the three ``` schneider50k_{phase}.csv ``` files, using the script ```rxnebm/data/preprocess/clean_smiles.py```, i.e. 
```
    python -m rxnebm.data.preprocess.clean_smiles
```
This data is included in this repository under ```rxnebm/data/cleaned_data/``` as ```50k_clean_rxnsmi_noreagent_allmapped_cano_{phase}.pickle```, and also in the [Google Drive](https://drive.google.com/drive/folders/1ISXFL7SuVY_sW3z36hQfpyDMH1KF22nS?usp=sharing) sub-folder ```Retro_Reproduction``` 
Specifically, we perform these steps:
1. Keep all atom mapping
2. Remove reaction SMILES strings with product molecules that are too small and clearly incorrect. The criteria used was ```len(prod_smi) < 3```. 4 reaction SMILES strings were caught by this criteria, with products: 		
    - ```'CN[C@H]1CC[C@@H](c2ccc(Cl)c(Cl)c2)c2ccc([I:19])cc21>>[IH:19]'```
    - ```'O=C(CO)N1CCC(C(=O)[OH:28])CC1>>[OH2:28]'```
    - ```'CC(=O)[Br:4]>>[BrH:4]'```
    - ```'O=C(Cn1c(-c2ccc(Cl)c(Cl)c2)nc2cccnc21)[OH:10]>>[OH2:10]'```
3. Remove all duplicate reaction SMILES strings
4. Remove reaction SMILES in the training data that overlap with validation/test sets + validation data that overlap with the test set.
    - test_appears_in_train: 50
    - test_appears_in_valid: 6
    - valid_appears_in_train: 44
5. Finally, we obtain an (extra) clean dataset of reaction SMILES:
    - Train: 39713
    - Valid: 4989
    - Test: 5005
6. Canonicalization: when it came to sequence models or certain retro models that are sensitive to how the reaction SMILES is
written, it came to our attention that it is important to ensure all the SMILES are standardized in some way. We simply use RDKit functions to achieve this. After running clean_smiles.py, we run canonicalize.py in the same folder:
    ```
        python -m rxnebm.data.preprocess.canonicalize
    ```
    For atom-mapped rxn_smi, there should be 35/4/4 rxn_smi in train/valid/test that are changed after RDKit canonicalization. Of course, this canonicalization won't be important for non sequence based models; still, it is highly recommended to do this.
    
### Re-ranking task: proposal data from retrosynthesis models re-trained on our cleaner USPTO-50K:

For each model in our model zoo, we generate the top-K predictions for each product SMILES in our extra-clean dataset. All of these files belong in ```rxnebm/data/cleaned_data ``` 
1. Retrosim, with top-200 predictions (using 200 maximum precedents for product similarity search): 
    - 3 CSV files of SMILES strings in the [Retrosim_proposals folder](https://drive.google.com/drive/folders/1HhzBwfa5Oykfxq11qM4oQIuw3ZGjJqYH). 
        This is first generated by running 
        
        ``` python -m rxnebm.proposer.retrosim_model ```
        
        This step takes 13 hours on an 8 core machine. You only need to run this python script again if you wish to get more than top-200 predictions, or beyond 200 max precedents, or modify the underlying RetroSim model; otherwise, just download it from our Google Drive using ``` download_data.py ```
    
       As a precautionary measure, we canonicalize all these precursors again and ensure no training reaction has duplicate ground-truth, by running:

       ``` bash scripts/retrosim/clean.sh ```
    
    - 3 .npz files of sparse reaction fingerprints ``` retrosim_rxn_fps_{phase}.npz ``` in the [datasets folder](https://drive.google.com/drive/folders/1ISXFL7SuVY_sW3z36hQfpyDMH1KF22nS). 
    
        This is generated by running
        
        ``` python gen_proposals/gen_fps_from_proposals.py --proposer retrosim --topk 50 --maxk 200 ```

        It takes about 8 minutes on a 32 core machine. Please refer to ``` scripts/retrosim/make_fp.sh ``` and ```gen_proposals/gen_fps_from_proposals.py``` themselves for detailed arguments.
        
        Since RetroSim will not generate the full 50/200 proposals for every product, we pad the reaction fingerprints with all-zero vectors for batching and mask these during training & testing.

    - 3 sets of graph features (1 for each phase). Each set consists of: ```cache_feat_index.npz```,        ```cache_feat.npz```, ```cache_mask.pkl```, ```cache_smi.pkl```. Note that these 3 sets in total take up    between 20 to 30 GBs, so ensure you have sufficient disk space. We again provide them in our Drive, but you can also generate them yourself using:
    
        ``` bash scripts/retrosim/make_graphfeat.sh ```

        It takes about 12 minutes on 32 cores.

    - As stated in our paper, just training on the top-50 proposals (```--topk 50```) is sufficient and yields the same performance as training on more predictions (e.g. top-100/200); for testing, we still keep the top-200 proposals (```--maxk 200```) to maximize the chances of the published reaction appearing in those 200 proposals for re-ranking.

    Once either the reaction fingerprints or the graphs have been generated, follow the instructions under ```Training``` above to train the EBMs.

2. GLN, with top-200 predictions (beam_size=200):
    - First we need to train GLN itself. We already include the 3 CSV files to train GLN, which contains the atom-mapped, canonicalized, extra-clean reaction SMILES from USPTO-50K, in 
    ``` rxnebm/proposer/gln_openretro/data/gln_schneider50k/ ```.
        - To make them, just run: <br>
        ``` python prep_data_for_retro_models.py --output_format gln```
    - We created a wrapper of the original GLN repo, to ease some issues installing GLN as a package, as well as standardize training, testing & proposing. Our official wrapper, **openretro**, is still under development to be released soon, and we include the GLN portion in this repo at: <br>
    ``` cd rxnebm/proposer/gln_openretro ```
    - To install GLN: once you're in ``` gln_openretro ```, run: <br> ``` bash scripts/setup.sh ``` <br> This creates a conda environment called ```gln_openretro```, which you need to activate to train/test/propose with GLN.
    - To preprocess training data: <br>
    ``` bash scripts/preprocess.sh ```
    - To train (takes ~2 hours on 1 RTX2080Ti): <br>
    ``` bash scripts/train.sh ```
    Note that you need to specify a training seed, which is set to ```77777777``` by default.
    - To test (takes ~4 hours on 1 RTX2080Ti, because it tests all 10 checkpoints): <br>
    ``` bash scripts/test.sh ```
    Testing is important because it tells you the best checkpoint (by validation top-1 accuracy) to use for proposing. On seed ```77777777```, this should be ```model-6.dump```. <br> 
    **TODO: try to automate this by returning some value which can be used by propose_and_compile.sh**
    - To propose, we need to go back up to root with: <br>
    ``` cd ../../../ ``` <br>
    Then run (takes ~12 hours on 1 RTX2080Ti): <br>
    ``` bash scripts/gln/propose_and_compile.sh ``` <br>
    You may need to modify the ``` gln_seed ``` and ``` best_ckpt ``` arguments within ``` propose_and_compile.sh ```. If your best checkpoint was ```model-6.dump```, then set ```best_ckpt=6```. 
    - The last step is to generate either the fingerprints or graphs. This step is very similar across all 4 proposers. 
        - Fingerprints: <br>
        ``` bash scripts/gln/make_fp.sh ```
        - Graphs: <br>
        ``` bash scripts/gln/make_graphfeat.sh ```
    - Finally, we can train the EBMs on GLN! Whew! That took a while. Alternatively, you can just grab the fingerprints and/or graphs off our Google Drive.


<!-- ## Appendix B - Misc details
This project uses ``` black ``` for auto-formatting to the ``` pep8 ``` style guide, and ``` isort ``` to sort imports. ``` pylint ``` is also used in ``` vscode ``` to lint all code. -->


<!-- ### Data preprocessing
The entire data preprocessing pipeline can be run from ``` prep_data_for_EBM.py ```, which performs a series of data cleaning & preprocessing steps, in the following order:
1. Cleans the raw SMILES strings
2. Extracts all unique molecule SMILES strings as a list
3. Converts unique molecule SMILES into a matrix of Morgan count molecular fingerprints.
4. Generates 2 lookup tables (dictionaries), 1 to map molecular SMILES strings into the corresponding index in that matrix of Morgan count fingerprints, and the other to map the reverse: index (key) -> molecular SMILES (value) 
5. Builds a nearest-neighbour search index using the [nmslib](https://github.com/DrrDom/crem) package
6. Generates a (very) large database of [CReM](https://github.com/DrrDom/crem) negatives, mapping each product SMILES string in the dataset (key), to a list of highly similar, mutated product SMILES strings (value). Note that this step can take from 10-13 hours on the USPTO_50k dataset for 150 mutated products / original product, and that CReM does not guarantee the requested number of mutated products. To deal with this, we pad with vectors of all 0's, and implement a simple masking step in our network to ignore these vectors. <br> -->

<!-- ### List of provided data for pre-training: -->
<!-- For ease of reproducibility, all data is [available on Google Drive](https://drive.google.com/drive/folders/1ISXFL7SuVY_sW3z36hQfpyDMH1KF22nS?usp=sharing). These belong in ```rxnebm/data/cleaned_data ```. -->
<!-- - three ```50k_clean_rxnsmi_noreagent_{phase}.pickle``` files contain the cleaned reaction SMILES strings from USPTO_50k (generated by ```rxnebm/data/preprocess/clean_smiles.py```)
- ```50k_mol_smis.pickle``` (list of all unique molecule SMILES, generated by ```rxnebm/data/preprocess/clean_smiles.py```) 
- ```50k_mol_smi_to_sparse_fp_idx.pickle``` (the lookup table mapping molecular SMILES to Morgan count molecular fingerprints by their index in the sparse matrix ```50k_count_mol_fps.npz```, generated by ```rxnebm/data/preprocess/smi_to_fp.py```) 
- ```50k_sparse_fp_idx_to_mol_smi.pickle``` (the lookup table mapping Morgan count molecular fingerprints by their index to molecular SMILES, generated by ```rxnebm/data/preprocess/smi_to_fp.py```) 
- ```50k_count_mol_fps.npz``` (matrix of Morgan count moelcular fingerprints, generated by ```rxnebm/data/preprocess/smi_to_fp.py```) 
- ```50k_cosine_count.bin``` & ```50k_cosine_count.bin.dat``` (generated by ```rxnebm/data/preprocess/prep_nmslib.py```)
- ```50k_neg150_rad2_maxsize3_mutprodsmis.pickle``` (generated by ```rxnebm/data/preprocess/prep_crem.py```). ```50k_neg150_rad2_maxsize3_insufficient.pickle``` is an optional (currently unused) dictionary mapping each product SMILES (key) for which CReM could not generate the requested number of mutated molecules, to the number of negatives (value) that CReM could generate. <br>
-  ```50k_rdm_5_cos_5_bit_5_1_1_mut_10_{split}.npz``` (a matrix of positive and augmented negative reaction fingerprints), the syntax being: ```{dataset_name}_rdm_{num_rdm_negs}_cos_{num_cos_negs}_bit_{num_bit_negs}_{num_bits}_{increment_bits}_mut_{num_mut_negs}_{split}.npz``` -->