# NOT checkpointing (to save space)
python3 trainEBM.py \
  --model_name="FeedforwardTriple3indiv3prod1cos" \
  --encoder_hidden_size 1024 128 \
  --encoder_dropout 0.2 \
  --encoder_activation "PReLU" \
  --out_hidden_sizes 128 \
  --out_activation "PReLU" \
  --out_dropout 0.2 \
  --rxn_smis_file_prefix="50k_clean_rxnsmi_noreagent" \
  --do_finetune \
  --do_test \
  --do_get_energies_and_acc \
	--log_file=FF_sym/1024x128_128_XPERT_50top200max_lr1e3_wd0_fac20_pat0_stop2_40ep_${SLURM_JOBID}.log \
	--expt_name=1024x128_128_XPERT_50top200max_lr1e3_wd0_fac20_pat0_stop2_40ep \
    --proposals_csv_file_prefix="retroxpert_200topk_200maxk_noGT" \
    --precomp_file_prefix="retroxpert_rxn_fps_50topk_200maxk_16384_hybrid_all" \
    --loss_type 'log' \
	--representation="fingerprint" \
	--random_seed=0 \
	--batch_size=32 \
    --batch_size_eval=32 \
    --lr_floor_stop_training \
    --lr_floor 8e-7 \
    --lr_cooldown=0 \
	--learning_rate=1e-3 \
    --weight_decay=0 \
    --lr_scheduler='ReduceLROnPlateau' \
    --lr_scheduler_factor=0.2 \
    --lr_scheduler_patience=0 \
	--optimizer="Adam" \
	--epochs=40 \
    --early_stop \
    --early_stop_patience=2 \
    --test_on_train
