{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMGN51DE3ZZg"
   },
   "source": [
    "### To do: \n",
    "- build Ball Tree for cosine similarity\n",
    "- implement Bayesian optimisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c1pxoCjT4CbP",
    "outputId": "4f7e429c-1150-4f7f-8d34-da78a60281ff"
   },
   "outputs": [],
   "source": [
    "# Install RDKit. Takes 2-3 minutes\n",
    "# !wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# !chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "# !time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "# !time conda install -q -y -c conda-forge python=3.7\n",
    "# !time conda install -q -y -c conda-forge rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "87OYfp5T4QDS",
    "outputId": "7297a371-2c64-4395-de18-8f8b821bd88e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8Q1rtww36JU"
   },
   "outputs": [],
   "source": [
    "# DRIVE_PATH_TO_PICKLE = '/content/gdrive/My Drive/rxn_ebm/USPTO_50k_Schneider/clean_rxn_50k_nomap_noreagent.pickle'\n",
    "# VM_PATH_TO_PICKLE = '/content/'\n",
    "\n",
    "# !cp '/content/gdrive/My Drive/rxn_ebm/USPTO_50k_Schneider/clean_rxn_50k_nomap_noreagent.pickle' '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSKTT4aD3ZZh"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/usr/local/lib/python3.7/site-packages/') \n",
    "# for Colab \n",
    "import os\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.ipython_useSVG=True\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import rdChemReactions\n",
    "from rdkit.Chem import rdqueries # faster than iterating atoms https://sourceforge.net/p/rdkit/mailman/message/34538007/ \n",
    "from rdkit.Chem.rdchem import Atom\n",
    "from rdkit import DataStructs\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re \n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMIpqIVk3ZZs"
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfFuy7g03ZZs"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_activation_function(activation: str) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Gets an activation function module given the name of the activation.\n",
    "    Supports:\n",
    "    * :code:`ReLU`\n",
    "    * :code:`LeakyReLU`\n",
    "    * :code:`PReLU`\n",
    "    * :code:`tanh`\n",
    "    * :code:`SELU`\n",
    "    * :code:`ELU`\n",
    "    :param activation: The name of the activation function.\n",
    "    :return: The activation function module.\n",
    "    \"\"\"\n",
    "    if activation == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'LeakyReLU':\n",
    "        return nn.LeakyReLU(0.1)\n",
    "    elif activation == 'PReLU':\n",
    "        return nn.PReLU()\n",
    "    elif activation == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif activation == 'SELU':\n",
    "        return nn.SELU()\n",
    "    elif activation == 'ELU':\n",
    "        return nn.ELU()\n",
    "    else:\n",
    "        raise ValueError(f'Activation \"{activation}\" not supported.')\n",
    "    \n",
    "def initialize_weights(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Initializes the weights of a model in place.\n",
    "    :param model: An PyTorch model.\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)\n",
    "            \n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1Cfy0AF3ZZv"
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxvJUWjr3ZZw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FF_ebm(nn.Module):\n",
    "    '''\n",
    "    trainargs: dictionary containing hyperparameters to be optimised, \n",
    "    hidden_sizes must be a list e.g. [1024, 512, 256]\n",
    "    \n",
    "    To do: bayesian optimisation\n",
    "    '''\n",
    "    def __init__(self, trainargs):\n",
    "        super(FF_ebm, self).__init__()\n",
    "        self.output_size = trainargs['output_size']\n",
    "        self.num_layers = len(trainargs['hidden_sizes']) + 1\n",
    "\n",
    "        if trainargs['model'] == 'FF_sep':\n",
    "          self.input_dim = trainargs['rctfp_size'] + trainargs['prodfp_size'] # will be rctfp_size + prodfp_size for FF_sep\n",
    "        elif trainargs['model'] == 'FF_diff':\n",
    "          self.input_dim = trainargs['rctfp_size']\n",
    "          assert trainargs['rctfp_size'] == trainargs['prodfp_size'], 'rctfp_size != prodfp_size, unable to make difference FPs!!!'\n",
    "\n",
    "        self.create_ffn(trainargs)\n",
    "        initialize_weights(self)  # is it necessary to initialize weights?? \n",
    "    \n",
    "    def create_ffn(self, trainargs):\n",
    "        '''\n",
    "        Creates feed-forward network using trainargs dict\n",
    "        '''\n",
    "        dropout = nn.Dropout(trainargs['dropout'])\n",
    "        activation = get_activation_function(trainargs['activation'])\n",
    "\n",
    "        if self.num_layers == 1:\n",
    "            ffn = [\n",
    "                dropout,\n",
    "                nn.Linear(self.input_dim, self.output_size)\n",
    "            ]\n",
    "        else:\n",
    "            ffn = [\n",
    "                dropout,\n",
    "                nn.Linear(self.input_dim, trainargs['hidden_sizes'][0])\n",
    "            ]\n",
    "            \n",
    "            # intermediate hidden layers \n",
    "            for i, layer in enumerate(range(self.num_layers - 2)):\n",
    "                ffn.extend([\n",
    "                    activation,\n",
    "                    dropout,\n",
    "                    nn.Linear(trainargs['hidden_sizes'][i], trainargs['hidden_sizes'][i+1]),\n",
    "                ])\n",
    "                \n",
    "            # last hidden layer\n",
    "            ffn.extend([\n",
    "                activation,\n",
    "                dropout,\n",
    "                nn.Linear(trainargs['hidden_sizes'][-1], self.output_size),\n",
    "            ])\n",
    "\n",
    "        self.ffn = nn.Sequential(*ffn)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        '''\n",
    "        Runs FF_ebm on input\n",
    "        \n",
    "        batch: a N x K x 1 tensor of N training samples, where each sample contains \n",
    "        a positive rxn on the first column, and K-1 negative rxn on subsequent columns \n",
    "        supplied by DataLoader on custom ReactionDataset \n",
    "        '''\n",
    "        energy_scores = self.ffn(batch) # tensor of size N x K x 1\n",
    "        return energy_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7EzzESW3ZZz"
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeAWhxLw3ZZz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class Run():\n",
    "    def __init__(self, model, trainargs,\n",
    "                 optimizer=None, load_checkpoint=False):\n",
    "        self.device = trainargs['device']\n",
    "        model = model.to(self.device)\n",
    "        self.model = model\n",
    "        if load_checkpoint: \n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = trainargs['optimizer'](model.parameters(), lr=trainargs['learning_rate'])\n",
    "        self.trainargs = trainargs \n",
    "\n",
    "        train_dataset = ReactionDataset(trainargs['path_to_pickle'], 'train', trainargs)\n",
    "        self.train_loader = DataLoader(train_dataset, trainargs['batch_size'], shuffle=True)\n",
    "        \n",
    "        val_dataset = ReactionDataset(trainargs['path_to_pickle'], 'valid', trainargs)\n",
    "        self.val_loader = DataLoader(val_dataset, 2 * trainargs['batch_size'], shuffle=False)\n",
    "        \n",
    "        test_dataset = ReactionDataset(self.trainargs['path_to_pickle'], 'test', self.trainargs)\n",
    "        self.test_loader = DataLoader(test_dataset, 2 * self.trainargs['batch_size'], shuffle=False)\n",
    "        del train_dataset, val_dataset, test_dataset\n",
    "\n",
    "        self.mean_train_loss = []\n",
    "        self.min_val_loss = 1e9\n",
    "        self.mean_val_loss = []\n",
    "        self.stats = {'trainargs': self.trainargs} # to store training statistics  \n",
    "\n",
    "        torch.manual_seed(trainargs['model_seed'])\n",
    "        random.seed(trainargs['random_seed'])\n",
    "    \n",
    "    def train_one(self, batch, val=False):\n",
    "        '''\n",
    "        Trains model for 1 epoch \n",
    "\n",
    "        TO DO: learning rate scheduler + logger \n",
    "        '''\n",
    "        self.model.zero_grad()\n",
    "        scores = self.model.forward(batch).squeeze(dim=-1) # scores: size N x K x 1 --> N x K after squeezing\n",
    "\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        probs = softmax(scores) # size N x K\n",
    "\n",
    "        # positives are the 0-th index of each sample, add a small epsilon 1e-9 to stabilise log \n",
    "        loss = -torch.log(probs[:, 0]+1e-9).mean() # probs[:, 0] is size N x 1 --> sum/mean to 1 value\n",
    "\n",
    "        if not val:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "        #     if args.grad_clip:\n",
    "        #         nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return loss.data.cpu()\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        Trains model for num_epochs provided in trainargs\n",
    "        Currently supports feed-forward networks: \n",
    "            FF_diff: takes as input a difference FP of fp_size & fp_radius\n",
    "            FF_sep: takes as input a concatenation of [reactants FP, product FP] \n",
    "\n",
    "        trainargs: dict of params \n",
    "        '''\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in np.arange(self.trainargs['epochs']):\n",
    "            self.model.train() # set model to training mode\n",
    "            train_loss = []\n",
    "            for batch in tqdm(self.train_loader): \n",
    "                batch = batch.to(self.device)\n",
    "                train_loss.append(self.train_one(batch, val=False))\n",
    "                self.mean_train_loss.append(np.mean(train_loss)) \n",
    "                # print('train_loss: {}'.format(train_loss))\n",
    "\n",
    "            self.model.eval() # validation mode\n",
    "            val_loss = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(self.val_loader):\n",
    "                    batch = batch.to(self.device)\n",
    "                    val_loss.append(self.train_one(batch, val=True))\n",
    "\n",
    "                if self.trainargs['early_stop'] and self.min_val_loss - np.mean(val_loss) < self.trainargs['min_delta']:\n",
    "                    if self.trainargs['patience'] <= wait:\n",
    "                        print('Early stopped at the end of epoch: ', epoch)\n",
    "                        print('mean_val_loss: ', np.mean(val_loss))\n",
    "                        stats['early_stop_epoch'] = epoch \n",
    "                        break \n",
    "                    else:\n",
    "                        wait += 1\n",
    "                        print('Decrease in val loss < min_delta, patience count: ', wait)\n",
    "                else:\n",
    "                    wait = 0\n",
    "                    self.min_val_loss = min(self.min_val_loss, np.mean(val_loss))\n",
    "                self.mean_val_loss.append(np.mean(val_loss))\n",
    "\n",
    "            if self.trainargs['checkpoint']: # adapted from moco: main_moco.py\n",
    "                save_checkpoint({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model': self.trainargs['model'],\n",
    "                        'state_dict': self.model.state_dict(),\n",
    "                        'optimizer' : self.optimizer.state_dict(),\n",
    "                        'stats' : self.stats,\n",
    "                    }, is_best=False, \n",
    "                    filename=self.trainargs['checkpoint_path']+'{}_{}_checkpoint_{:04d}.pth.tar'.format(\n",
    "                        self.trainargs['model'], self.trainargs['expt_name'], epoch))\n",
    "\n",
    "            print('Epoch: {}, train_loss: {}, val_loss: {}'.format(epoch, \n",
    "                                             np.around(np.mean(train_loss), decimals=4), \n",
    "                                             np.around(np.mean(val_loss), decimals=4)))\n",
    "\n",
    "        self.stats['mean_train_loss'] = self.mean_train_loss\n",
    "        self.stats['mean_val_loss'] = self.mean_val_loss\n",
    "        self.stats['min_val_loss'] = self.min_val_loss\n",
    "        self.stats['train_time'] = time.time() - start \n",
    "        # save training stats\n",
    "        torch.save(self.stats, self.trainargs['checkpoint_path']+'{}_{}_stats.pkl'.format(\n",
    "            self.trainargs['model'], self.trainargs['expt_name']))\n",
    "\n",
    "    def test(self):\n",
    "        '''\n",
    "        Evaluates the model on the test set \n",
    "        '''\n",
    "        test_loss = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader):\n",
    "                batch = batch.to(self.device)\n",
    "                test_loss.append(self.train_one(batch, val=True))\n",
    "\n",
    "        self.stats['test_loss'] = test_loss \n",
    "        self.stats['mean_test_loss'] = np.mean(test_loss)\n",
    "        print('train_time: {}'.format(self.stats['train_time']))\n",
    "        print('test_loss: {}'.format(self.stats['test_loss']))\n",
    "        print('mean_test_loss: {}'.format(self.stats['mean_test_loss']))\n",
    "        # overrides training stats w/ training + test stats\n",
    "        torch.save(self.stats, self.trainargs['checkpoint_path']+'{}_{}_stats.pkl'.format(\n",
    "            self.trainargs['model'], self.trainargs['expt_name'])) \n",
    "\n",
    "    def get_scores(self, dataloader):\n",
    "        ''' \n",
    "        Gets raw energy values (scores) from a trained model on a given dataloader\n",
    "        '''\n",
    "        scores = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "          for batch in tqdm(dataloader):\n",
    "              batch = batch.to(self.device)\n",
    "              self.model.zero_grad()\n",
    "              scores.append(self.model.forward(batch).squeeze(dim=-1)) \n",
    "            # scores: size N x K x 1 --> N x K after squeezing\n",
    "\n",
    "        return torch.cat(scores, dim=0)\n",
    "\n",
    "    def get_topk_acc(self, dataloader, k=1):\n",
    "        '''\n",
    "        Computes top-k accuracy of trained model in classifying feasible vs infeasible chemical rxns\n",
    "        (i.e. maximum energy score assigned to label 0 of each training sample) \n",
    "        '''\n",
    "        self.model.eval()\n",
    "        scores = self.get_scores(dataloader)\n",
    "        predicted_labels = torch.topk(scores, k, dim=1)[1]\n",
    "        \n",
    "        return torch.where(predicted_labels == 0)[0].shape[0]/predicted_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_o12DGGe3ZZ2"
   },
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYdV62U63ZZ3"
   },
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/tutorials/blob/master/beginner_source/data_loading_tutorial.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdChemReactions\n",
    "from rdkit import DataStructs\n",
    "import numpy as np\n",
    "\n",
    "def create_rxn_MorganFP(rxn_smi, fp_type='diff', radius=2, \n",
    "                        rctfp_size=16384, prodfp_size=16384, \n",
    "                        useChirality=True, dtype='int8'):\n",
    "    '''\n",
    "    fp_type: 'diff' or 'sep', \n",
    "    'diff' (difference):\n",
    "    Creates reaction MorganFP following Schneider et al in J. Chem. Inf. Model. 2015, 55, 1, 39–53\n",
    "    reactionFP = productFP - sum(reactantFPs)\n",
    "    \n",
    "    'sep' (separate):\n",
    "    Creates separate reactantsFP and productFP following Gao et al in ACS Cent. Sci. 2018, 4, 11, 1465–1476\n",
    "    '''\n",
    "    # initialise empty fp numpy arrays\n",
    "    if fp_type == 'diff':\n",
    "        diff_fp = np.empty(rctfp_size, dtype = dtype)\n",
    "    elif fp_type == 'sep':\n",
    "        rcts_fp = np.empty(rctfp_size, dtype = dtype)\n",
    "        prod_fp = np.empty(prodfp_size, dtype = dtype)\n",
    "    else:\n",
    "        print('ERROR: fp_type not recognised!')\n",
    "        return\n",
    "    \n",
    "    # create product FP\n",
    "    prod_mol = Chem.MolFromSmiles(rxn_smi.split('>')[-1])\n",
    "    try:\n",
    "        prod_fp_bit = AllChem.GetMorganFingerprintAsBitVect(\n",
    "                        mol=prod_mol, radius=radius, nBits=prodfp_size, useChirality=useChirality)\n",
    "\n",
    "        fp = np.empty(prodfp_size, dtype = dtype)   # temporarily store numpy array as fp \n",
    "        DataStructs.ConvertToNumpyArray(prod_fp_bit, fp)\n",
    "        if fp_type == 'diff':\n",
    "            diff_fp += fp\n",
    "        elif fp_type == 'sep':\n",
    "            prod_fp = fp\n",
    "    except Exception as e:\n",
    "        print(\"Cannot build product fp due to {}\".format(e))\n",
    "        return\n",
    "                                  \n",
    "    # create reactant FPs, subtracting each from product FP\n",
    "    rcts_smi = rxn_smi.split('>')[0].split('.')\n",
    "    for rct_smi in rcts_smi:\n",
    "        rct_mol = Chem.MolFromSmiles(rct_smi)\n",
    "        try:\n",
    "            rct_fp_bit = AllChem.GetMorganFingerprintAsBitVect(\n",
    "                            mol=rct_mol, radius=radius, nBits=rctfp_size, useChirality=useChirality)\n",
    "            fp = np.empty(rctfp_size, dtype = dtype)\n",
    "            DataStructs.ConvertToNumpyArray(rct_fp_bit, fp)\n",
    "            if fp_type == 'diff':\n",
    "                diff_fp -= fp\n",
    "            elif fp_type == 'sep':\n",
    "                rcts_fp += fp\n",
    "        except Exception as e:\n",
    "            print(\"Cannot build reactant fp due to {}\".format(e))\n",
    "            return\n",
    "    \n",
    "    if fp_type == 'diff':\n",
    "        return diff_fp\n",
    "    elif fp_type == 'sep':\n",
    "        return np.concatenate([rcts_fp, prod_fp])\n",
    "\n",
    "    \n",
    "class ReactionDataset(Dataset):\n",
    "    '''\n",
    "    The Dataset class ReactionDataset prepares training samples of length K: \n",
    "    [pos_rxn, neg_rxn_1, ..., neg_rxn_K-1], ... where K-1 = num_neg \n",
    "\n",
    "    TO DO: can this be further optimised? Augmentation is the clear bottleneck during training\n",
    "    '''\n",
    "    def __init__(self, path_to_pickle, key, trainargs):\n",
    "        '''\n",
    "        pickle is dict w/ keys 'train', 'valid', 'test' each storing a list of rxn_smiles (str)\n",
    "        IMPORTANT: molAtomMapNumbers have been cleared during data pre-processing \n",
    "        ''' \n",
    "        # feels like loading the entire pickle is not feasible when the dataset gets larger \n",
    "        # is there a more memory-efficient way to do this? \n",
    "        with open(path_to_pickle, 'rb') as handle: \n",
    "            self.rxn_smiles = pickle.load(handle)[key] \n",
    "        self.fp_radius = trainargs['fp_radius']\n",
    "        self.fp_type = trainargs['fp_type']\n",
    "        self.rctfp_size = trainargs['rctfp_size']\n",
    "        self.prodfp_size = trainargs['prodfp_size']\n",
    "        self.num_neg = trainargs['num_neg']\n",
    "    \n",
    "    def random_sample_negative(self, pos_rxn_smi, pos_rxn_idx):\n",
    "        '''\n",
    "        Generates 1 negative reaction given a positive reaction SMILES\n",
    "        Returns neg_rxn_smi (str)\n",
    "        '''\n",
    "        rcts_smi = pos_rxn_smi.split('>')[0].split('.')\n",
    "        prod_smi = pos_rxn_smi.split('>')[-1]       \n",
    "            \n",
    "        rct_or_prod = random.choice([0, 1])\n",
    "        if rct_or_prod == 0: # randomly change one of the reactant(s)\n",
    "            orig_idx = random.choice(np.arange(len(rcts_smi))) # randomly choose 1 reactant to be replaced\n",
    "            \n",
    "            found = False\n",
    "            while not found: # searches randomly to find a different rct molecule to swap with \n",
    "                rdm_rxn_idx = random.choice(np.arange(len(self.rxn_smiles))) # randomly choose 1 rxn\n",
    "                if rdm_rxn_idx == pos_rxn_idx: continue # don't choose the original rxn\n",
    "                        \n",
    "                new_rxn_smi = self.rxn_smiles[rdm_rxn_idx]\n",
    "                new_rcts_smi = new_rxn_smi.split('>')[0].split('.')\n",
    "\n",
    "                rdm_rcts_idx = random.choice(np.arange(len(new_rcts_smi)))\n",
    "                if new_rcts_smi[rdm_rcts_idx] != rcts_smi[orig_idx]:\n",
    "                    found = True\n",
    "                    rcts_smi[orig_idx] = new_rcts_smi[rdm_rcts_idx]\n",
    "            \n",
    "        else: # randomly change the product            \n",
    "            found = False\n",
    "            while not found:  # searches randomly to find a different prod molecule to swap with \n",
    "                rdm_rxn_idx = random.choice(np.arange(len(self.rxn_smiles)))\n",
    "                if rdm_rxn_idx == pos_rxn_idx: continue # don't choose the original rxn\n",
    "                        \n",
    "                new_rxn_smi = self.rxn_smiles[rdm_rxn_idx]      \n",
    "                new_prod_smi = new_rxn_smi.split('>')[-1]\n",
    "                if new_prod_smi != prod_smi:\n",
    "                    found = True\n",
    "                    prod_smi = new_prod_smi\n",
    "        \n",
    "        return '{}>>{}'.format('.'.join(rcts_smi), prod_smi)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "      ''' \n",
    "      Returns 1 training sample in the form [pos_rxn, neg_rxn_1, ..., neg_rxn_K-1]\n",
    "      num_neg: a hyperparameter to be tuned\n",
    "      '''\n",
    "      if torch.is_tensor(idx): # may not be needed, taken from data loading tutorial\n",
    "          idx = idx.tolist() \n",
    "\n",
    "      pos_rxn_smi = self.rxn_smiles[idx]\n",
    "      pos_rxn_fp = create_rxn_MorganFP(pos_rxn_smi, radius=self.fp_radius, \n",
    "                                      rctfp_size=self.rctfp_size, prodfp_size=self.prodfp_size, fp_type=self.fp_type)\n",
    "      \n",
    "      assert self.num_neg > 0, 'num_neg cannot be negative!'\n",
    "      neg_rxn_smis = [self.random_sample_negative(pos_rxn_smi, idx) for i in range(self.num_neg)]\n",
    "      neg_rxn_fps = [create_rxn_MorganFP(neg_rxn_smi, radius=self.fp_radius,  \n",
    "                                        rctfp_size=self.rctfp_size, prodfp_size=self.prodfp_size, fp_type=self.fp_type)\n",
    "                    for neg_rxn_smi in neg_rxn_smis]\n",
    "\n",
    "      return torch.Tensor([pos_rxn_fp, *neg_rxn_fps])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rxn_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ti5ndWB3ZaA"
   },
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "890bgQ783ZaB"
   },
   "outputs": [],
   "source": [
    "trainargs = {\n",
    "    'model': 'FF_diff', # must change both model & fp_type \n",
    "    'hidden_sizes': [256],  \n",
    "    'output_size': 1,\n",
    "    'dropout': 0.3, # adapted from Reaction Condition Recommender   \n",
    "    \n",
    "    'batch_size': 512,\n",
    "    'activation': 'ReLU', # trying ELU for its differentiability everywhere (vs ReLU which is not differentiable at x=0)\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'learning_rate': 5e-5, # to try: integrate w/ fast.ai lr_finder & lr_schedulers \n",
    "    'epochs': 50,\n",
    "    'early_stop': True,\n",
    "    'min_delta': 1e-5, \n",
    "    'patience': 3,\n",
    "\n",
    "    'checkpoint': True,\n",
    "    'model_seed': 1337,\n",
    "    'random_seed': 0, # affects neg rxn sampling since it is random\n",
    "    \n",
    "    'rctfp_size': 1024, # if fp_type == 'diff', ensure that both rctfp_size & prodfp_size are identical!\n",
    "    'prodfp_size': 1024,\n",
    "    'fp_radius': 2,\n",
    "    'fp_type': 'diff',\n",
    "    \n",
    "    'num_neg': 1, # to be tuned, 9 seems to be superior to 5 (overfitting occured quickly)\n",
    "    \n",
    "    'path_to_pickle': os.getcwd()+'/clean_rxn_50k_nomap_noreagent.pickle', \n",
    "    'checkpoint_path': os.getcwd()+'/checkpoints/',\n",
    "    'expt_name': 'test',\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "9mlGGCOZBEz4",
    "outputId": "93c963dd-f709-4f35-991a-880268233e72"
   },
   "outputs": [],
   "source": [
    "# initialises fingerprint-based feedforward EBM model \n",
    "model = FF_ebm(trainargs)\n",
    "run = Run(model, trainargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oAQVPmtNBAuo",
    "outputId": "d4d3cb35-84d5-4104-ce9b-b67a88db7d06"
   },
   "outputs": [],
   "source": [
    "run.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfmFtR4qBjrf"
   },
   "outputs": [],
   "source": [
    "run.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_o3lhYIioRA"
   },
   "outputs": [],
   "source": [
    "run.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0R5fkcaEBkV5"
   },
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "display.Audio(url=\"https://static.sfdict.com/audio/C07/C0702600.mp3\", autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load trained models from prev expts, & get topk accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trainargs': {'model': 'FF_sep',\n",
       "  'hidden_sizes': [256],\n",
       "  'output_size': 1,\n",
       "  'dropout': 0.5,\n",
       "  'batch_size': 512,\n",
       "  'activation': 'ReLU',\n",
       "  'optimizer': torch.optim.adam.Adam,\n",
       "  'learning_rate': 1e-05,\n",
       "  'epochs': 50,\n",
       "  'early_stop': True,\n",
       "  'min_delta': 1e-05,\n",
       "  'patience': 3,\n",
       "  'checkpoint': True,\n",
       "  'model_seed': 1337,\n",
       "  'random_seed': 0,\n",
       "  'rctfp_size': 2048,\n",
       "  'prodfp_size': 2048,\n",
       "  'fp_radius': 2,\n",
       "  'fp_type': 'sep',\n",
       "  'num_neg': 9,\n",
       "  'path_to_pickle': '/content/clean_rxn_50k_nomap_noreagent.pickle',\n",
       "  'checkpoint_path': '/content/gdrive/My Drive/rxn_ebm/',\n",
       "  'expt_name': '2048_1layer_9neg_rad2_ReLU'},\n",
       " 'early_stop_epoch': 15,\n",
       " 'mean_train_loss': [12.479294,\n",
       "  11.597199,\n",
       "  11.315463,\n",
       "  11.207648,\n",
       "  11.078769,\n",
       "  10.98713,\n",
       "  11.030012,\n",
       "  11.107498,\n",
       "  11.272827,\n",
       "  11.2081175,\n",
       "  11.163873,\n",
       "  11.160275,\n",
       "  11.181705,\n",
       "  11.137172,\n",
       "  11.099286,\n",
       "  11.139931,\n",
       "  11.090733,\n",
       "  11.087778,\n",
       "  11.052986,\n",
       "  11.010392,\n",
       "  10.987789,\n",
       "  10.988465,\n",
       "  10.971515,\n",
       "  10.981416,\n",
       "  10.945549,\n",
       "  10.928935,\n",
       "  10.928111,\n",
       "  10.907656,\n",
       "  10.943026,\n",
       "  10.950113,\n",
       "  10.966667,\n",
       "  10.942117,\n",
       "  10.911534,\n",
       "  10.904822,\n",
       "  10.9122505,\n",
       "  10.92788,\n",
       "  10.895492,\n",
       "  10.888226,\n",
       "  10.884,\n",
       "  10.871637,\n",
       "  10.8538265,\n",
       "  10.878697,\n",
       "  10.880023,\n",
       "  10.866846,\n",
       "  10.85989,\n",
       "  10.872579,\n",
       "  10.861169,\n",
       "  10.876737,\n",
       "  10.864556,\n",
       "  10.846026,\n",
       "  10.844303,\n",
       "  10.848845,\n",
       "  10.848232,\n",
       "  10.8256,\n",
       "  10.825517,\n",
       "  10.816744,\n",
       "  10.804302,\n",
       "  10.790392,\n",
       "  10.795384,\n",
       "  10.797846,\n",
       "  10.782581,\n",
       "  10.7999935,\n",
       "  10.789489,\n",
       "  10.781935,\n",
       "  10.772123,\n",
       "  10.7591095,\n",
       "  10.744498,\n",
       "  10.730766,\n",
       "  10.717055,\n",
       "  10.718441,\n",
       "  10.726357,\n",
       "  10.704991,\n",
       "  10.698103,\n",
       "  10.685357,\n",
       "  10.672092,\n",
       "  10.660067,\n",
       "  10.645772,\n",
       "  10.657561,\n",
       "  10.648633,\n",
       "  9.901606,\n",
       "  9.764109,\n",
       "  9.773925,\n",
       "  10.192066,\n",
       "  10.055987,\n",
       "  9.970162,\n",
       "  10.096589,\n",
       "  10.054543,\n",
       "  9.986362,\n",
       "  9.974589,\n",
       "  10.021478,\n",
       "  10.0390625,\n",
       "  10.082708,\n",
       "  10.002287,\n",
       "  10.003145,\n",
       "  10.0398245,\n",
       "  10.010846,\n",
       "  9.968725,\n",
       "  9.943683,\n",
       "  9.969751,\n",
       "  9.950761,\n",
       "  9.9203615,\n",
       "  9.9613905,\n",
       "  9.94381,\n",
       "  9.94537,\n",
       "  9.949209,\n",
       "  9.951069,\n",
       "  9.986678,\n",
       "  10.037173,\n",
       "  10.015714,\n",
       "  9.983801,\n",
       "  9.976755,\n",
       "  9.949626,\n",
       "  9.950081,\n",
       "  9.949718,\n",
       "  9.965643,\n",
       "  9.939631,\n",
       "  9.9214735,\n",
       "  9.916232,\n",
       "  9.944681,\n",
       "  9.930865,\n",
       "  9.924301,\n",
       "  9.913006,\n",
       "  9.91986,\n",
       "  9.918188,\n",
       "  9.910044,\n",
       "  9.921495,\n",
       "  9.914864,\n",
       "  9.909409,\n",
       "  9.901319,\n",
       "  9.892731,\n",
       "  9.8686285,\n",
       "  9.854333,\n",
       "  9.84018,\n",
       "  9.828837,\n",
       "  9.805931,\n",
       "  9.795322,\n",
       "  9.780904,\n",
       "  9.781767,\n",
       "  9.799349,\n",
       "  9.781175,\n",
       "  9.774357,\n",
       "  9.799629,\n",
       "  9.778038,\n",
       "  9.787738,\n",
       "  9.785748,\n",
       "  9.791719,\n",
       "  9.7961855,\n",
       "  9.78146,\n",
       "  9.781731,\n",
       "  9.777334,\n",
       "  9.771442,\n",
       "  9.780002,\n",
       "  9.76767,\n",
       "  9.757435,\n",
       "  9.762751,\n",
       "  9.754181,\n",
       "  9.747216,\n",
       "  9.741048,\n",
       "  8.95925,\n",
       "  9.094856,\n",
       "  9.122019,\n",
       "  9.071448,\n",
       "  8.999968,\n",
       "  8.882172,\n",
       "  8.81917,\n",
       "  8.661304,\n",
       "  8.673784,\n",
       "  8.612231,\n",
       "  8.648464,\n",
       "  8.647774,\n",
       "  8.654147,\n",
       "  8.647798,\n",
       "  8.760171,\n",
       "  8.809517,\n",
       "  8.786673,\n",
       "  8.773136,\n",
       "  8.736482,\n",
       "  8.725542,\n",
       "  8.717137,\n",
       "  8.712582,\n",
       "  8.723553,\n",
       "  8.713672,\n",
       "  8.71481,\n",
       "  8.705031,\n",
       "  8.737762,\n",
       "  8.716661,\n",
       "  8.7479105,\n",
       "  8.72493,\n",
       "  8.7461815,\n",
       "  8.728319,\n",
       "  8.700218,\n",
       "  8.744722,\n",
       "  8.740013,\n",
       "  8.73747,\n",
       "  8.734722,\n",
       "  8.729634,\n",
       "  8.7162285,\n",
       "  8.689878,\n",
       "  8.679882,\n",
       "  8.663596,\n",
       "  8.66518,\n",
       "  8.661142,\n",
       "  8.632384,\n",
       "  8.647249,\n",
       "  8.635171,\n",
       "  8.634282,\n",
       "  8.637642,\n",
       "  8.6289625,\n",
       "  8.62213,\n",
       "  8.616402,\n",
       "  8.612842,\n",
       "  8.602836,\n",
       "  8.589102,\n",
       "  8.580899,\n",
       "  8.570464,\n",
       "  8.566766,\n",
       "  8.559215,\n",
       "  8.552717,\n",
       "  8.547064,\n",
       "  8.531298,\n",
       "  8.535728,\n",
       "  8.545139,\n",
       "  8.540627,\n",
       "  8.558139,\n",
       "  8.544183,\n",
       "  8.541186,\n",
       "  8.535279,\n",
       "  8.524864,\n",
       "  8.513535,\n",
       "  8.505571,\n",
       "  8.495569,\n",
       "  8.484087,\n",
       "  8.480893,\n",
       "  8.479007,\n",
       "  8.483144,\n",
       "  8.478802,\n",
       "  8.481474,\n",
       "  7.8394446,\n",
       "  8.20864,\n",
       "  8.078637,\n",
       "  8.26299,\n",
       "  8.083304,\n",
       "  8.141108,\n",
       "  8.193605,\n",
       "  8.117257,\n",
       "  8.031652,\n",
       "  8.130587,\n",
       "  8.11996,\n",
       "  8.078074,\n",
       "  8.064733,\n",
       "  8.061302,\n",
       "  8.044612,\n",
       "  8.087645,\n",
       "  8.054756,\n",
       "  8.053225,\n",
       "  8.084149,\n",
       "  8.061744,\n",
       "  8.056603,\n",
       "  8.072048,\n",
       "  8.030645,\n",
       "  8.034971,\n",
       "  8.017377,\n",
       "  8.005487,\n",
       "  8.005205,\n",
       "  8.039966,\n",
       "  8.031323,\n",
       "  8.014539,\n",
       "  8.029863,\n",
       "  8.002653,\n",
       "  7.9920006,\n",
       "  8.005033,\n",
       "  7.994425,\n",
       "  7.9987082,\n",
       "  8.018143,\n",
       "  8.009313,\n",
       "  8.004737,\n",
       "  7.993904,\n",
       "  7.9975863,\n",
       "  7.996166,\n",
       "  7.980769,\n",
       "  7.9750686,\n",
       "  7.9764175,\n",
       "  7.981822,\n",
       "  7.9901915,\n",
       "  7.9832115,\n",
       "  7.980542,\n",
       "  7.9669886,\n",
       "  7.9632,\n",
       "  7.9654093,\n",
       "  7.961264,\n",
       "  7.9518123,\n",
       "  7.9544773,\n",
       "  7.965043,\n",
       "  7.970698,\n",
       "  7.964309,\n",
       "  7.9712267,\n",
       "  7.9666376,\n",
       "  7.9611473,\n",
       "  7.959498,\n",
       "  7.9685307,\n",
       "  7.9559736,\n",
       "  7.9717116,\n",
       "  7.970125,\n",
       "  7.95848,\n",
       "  7.9510427,\n",
       "  7.954136,\n",
       "  7.9611487,\n",
       "  7.9546947,\n",
       "  7.948983,\n",
       "  7.9437213,\n",
       "  7.939402,\n",
       "  7.9497647,\n",
       "  7.9608116,\n",
       "  7.954529,\n",
       "  7.9544225,\n",
       "  7.9630504,\n",
       "  7.684237,\n",
       "  7.76853,\n",
       "  7.798565,\n",
       "  7.6583056,\n",
       "  7.801474,\n",
       "  7.6913567,\n",
       "  7.675016,\n",
       "  7.725437,\n",
       "  7.6594715,\n",
       "  7.601799,\n",
       "  7.650574,\n",
       "  7.6565,\n",
       "  7.7267737,\n",
       "  7.6485987,\n",
       "  7.701965,\n",
       "  7.7483816,\n",
       "  7.744506,\n",
       "  7.722531,\n",
       "  7.730478,\n",
       "  7.7960157,\n",
       "  7.837037,\n",
       "  7.88571,\n",
       "  7.859814,\n",
       "  7.8413205,\n",
       "  7.839412,\n",
       "  7.824525,\n",
       "  7.831036,\n",
       "  7.844501,\n",
       "  7.849064,\n",
       "  7.8720617,\n",
       "  7.892184,\n",
       "  7.878392,\n",
       "  7.882338,\n",
       "  7.870985,\n",
       "  7.8693857,\n",
       "  7.8583364,\n",
       "  7.855514,\n",
       "  7.84028,\n",
       "  7.8281407,\n",
       "  7.8555145,\n",
       "  7.840521,\n",
       "  7.85017,\n",
       "  7.841771,\n",
       "  7.8510027,\n",
       "  7.824956,\n",
       "  7.8140798,\n",
       "  7.7995324,\n",
       "  7.797191,\n",
       "  7.794482,\n",
       "  7.8000226,\n",
       "  7.788639,\n",
       "  7.8038383,\n",
       "  7.8117585,\n",
       "  7.8082705,\n",
       "  7.8019605,\n",
       "  7.8073854,\n",
       "  7.806635,\n",
       "  7.7938485,\n",
       "  7.782881,\n",
       "  7.7915955,\n",
       "  7.799546,\n",
       "  7.804425,\n",
       "  7.791497,\n",
       "  7.7965293,\n",
       "  7.7955647,\n",
       "  7.79612,\n",
       "  7.8080106,\n",
       "  7.8046174,\n",
       "  7.806953,\n",
       "  7.8079424,\n",
       "  7.804426,\n",
       "  7.808526,\n",
       "  7.8136497,\n",
       "  7.7994394,\n",
       "  7.800142,\n",
       "  7.7958198,\n",
       "  7.7878623,\n",
       "  7.7842407,\n",
       "  7.7724695,\n",
       "  7.2110996,\n",
       "  7.508336,\n",
       "  7.55267,\n",
       "  7.4149876,\n",
       "  7.39453,\n",
       "  7.4026475,\n",
       "  7.299355,\n",
       "  7.367658,\n",
       "  7.3235955,\n",
       "  7.2418823,\n",
       "  7.230156,\n",
       "  7.284042,\n",
       "  7.2606106,\n",
       "  7.2426453,\n",
       "  7.257721,\n",
       "  7.271323,\n",
       "  7.256474,\n",
       "  7.2265005,\n",
       "  7.2200637,\n",
       "  7.2690187,\n",
       "  7.293545,\n",
       "  7.328819,\n",
       "  7.308602,\n",
       "  7.299254,\n",
       "  7.3281,\n",
       "  7.343513,\n",
       "  7.3389244,\n",
       "  7.32614,\n",
       "  7.3195477,\n",
       "  7.309531,\n",
       "  7.307011,\n",
       "  7.301404,\n",
       "  7.3436675,\n",
       "  7.347915,\n",
       "  7.3457355,\n",
       "  7.365787,\n",
       "  7.365266,\n",
       "  7.358638,\n",
       "  7.3599916,\n",
       "  7.364615,\n",
       "  7.3770795,\n",
       "  7.3976455,\n",
       "  7.374952,\n",
       "  7.3777294,\n",
       "  7.368106,\n",
       "  7.384708,\n",
       "  7.3659744,\n",
       "  7.3803177,\n",
       "  7.3766375,\n",
       "  7.403503,\n",
       "  7.3872323,\n",
       "  7.414527,\n",
       "  7.3942094,\n",
       "  7.421781,\n",
       "  7.4188504,\n",
       "  7.431545,\n",
       "  7.425039,\n",
       "  7.4300785,\n",
       "  7.417299,\n",
       "  7.4290667,\n",
       "  7.4203863,\n",
       "  7.4174395,\n",
       "  7.4048977,\n",
       "  7.398278,\n",
       "  7.410669,\n",
       "  7.4238467,\n",
       "  7.4220567,\n",
       "  7.418278,\n",
       "  7.426014,\n",
       "  7.430802,\n",
       "  7.436923,\n",
       "  7.439984,\n",
       "  7.450686,\n",
       "  7.449434,\n",
       "  7.4421363,\n",
       "  7.4363112,\n",
       "  7.4296913,\n",
       "  7.427936,\n",
       "  7.4358854,\n",
       "  7.1116066,\n",
       "  6.824835,\n",
       "  6.821274,\n",
       "  7.2562537,\n",
       "  7.259126,\n",
       "  7.3029504,\n",
       "  7.288834,\n",
       "  7.222161,\n",
       "  7.2531967,\n",
       "  7.1924696,\n",
       "  7.1590056,\n",
       "  7.152637,\n",
       "  7.1926427,\n",
       "  7.1969404,\n",
       "  7.185521,\n",
       "  7.1292343,\n",
       "  7.1400056,\n",
       "  7.1230783,\n",
       "  7.1562395,\n",
       "  7.1551237,\n",
       "  7.148064,\n",
       "  7.157997,\n",
       "  7.1667967,\n",
       "  7.23391,\n",
       "  7.293064,\n",
       "  7.3338165,\n",
       "  7.375812,\n",
       "  7.368418,\n",
       "  7.3383684,\n",
       "  7.31397,\n",
       "  7.300112,\n",
       "  7.3315573,\n",
       "  7.3341575,\n",
       "  7.3245,\n",
       "  7.314333,\n",
       "  7.305458,\n",
       "  7.2889833,\n",
       "  7.2844534,\n",
       "  7.2858925,\n",
       "  7.3081408,\n",
       "  7.302214,\n",
       "  7.302182,\n",
       "  7.300677,\n",
       "  7.3037205,\n",
       "  7.2797112,\n",
       "  7.257063,\n",
       "  7.281618,\n",
       "  7.2775016,\n",
       "  7.2709327,\n",
       "  7.281178,\n",
       "  7.26308,\n",
       "  7.256262,\n",
       "  7.2425723,\n",
       "  7.2306337,\n",
       "  7.2318325,\n",
       "  7.220757,\n",
       "  7.212293,\n",
       "  7.2312837,\n",
       "  7.224781,\n",
       "  7.2239194,\n",
       "  7.2258005,\n",
       "  7.244549,\n",
       "  7.239551,\n",
       "  7.241935,\n",
       "  7.235899,\n",
       "  7.243558,\n",
       "  7.2463894,\n",
       "  7.2384706,\n",
       "  7.2393937,\n",
       "  7.2402925,\n",
       "  7.2403584,\n",
       "  7.2385426,\n",
       "  7.2459087,\n",
       "  7.2445116,\n",
       "  7.246827,\n",
       "  7.25073,\n",
       "  7.242201,\n",
       "  7.23595,\n",
       "  7.2307606,\n",
       "  7.181249,\n",
       "  6.8635645,\n",
       "  6.8866143,\n",
       "  7.0872016,\n",
       "  7.0643377,\n",
       "  7.0829544,\n",
       "  7.2816496,\n",
       "  7.2262964,\n",
       "  7.161549,\n",
       "  7.136512,\n",
       "  7.1152844,\n",
       "  7.0913367,\n",
       "  7.112675,\n",
       "  7.1233187,\n",
       "  7.1151223,\n",
       "  7.221798,\n",
       "  7.2038574,\n",
       "  7.2107544,\n",
       "  7.1715875,\n",
       "  7.1451087,\n",
       "  7.1517544,\n",
       "  7.1207843,\n",
       "  7.155406,\n",
       "  7.141716,\n",
       "  7.12314,\n",
       "  7.109129,\n",
       "  7.1140704,\n",
       "  7.0851226,\n",
       "  7.1276193,\n",
       "  7.1142664,\n",
       "  7.1557317,\n",
       "  7.145009,\n",
       "  7.1798463,\n",
       "  7.1703763,\n",
       "  7.1618247,\n",
       "  7.174102,\n",
       "  7.1946297,\n",
       "  7.19377,\n",
       "  7.181642,\n",
       "  7.1951814,\n",
       "  7.2239976,\n",
       "  7.2192464,\n",
       "  7.214355,\n",
       "  7.209158,\n",
       "  7.1914997,\n",
       "  7.1956773,\n",
       "  7.2191615,\n",
       "  7.213678,\n",
       "  7.234887,\n",
       "  7.238772,\n",
       "  7.2532234,\n",
       "  7.26243,\n",
       "  7.2900643,\n",
       "  7.298087,\n",
       "  7.282328,\n",
       "  7.303094,\n",
       "  7.3277807,\n",
       "  7.318191,\n",
       "  7.3303967,\n",
       "  7.322009,\n",
       "  7.3340187,\n",
       "  7.349501,\n",
       "  7.366358,\n",
       "  7.3522377,\n",
       "  7.3624935,\n",
       "  7.352824,\n",
       "  7.3440804,\n",
       "  7.3476963,\n",
       "  7.362523,\n",
       "  7.365004,\n",
       "  7.354474,\n",
       "  7.3493786,\n",
       "  7.359425,\n",
       "  7.361441,\n",
       "  7.36043,\n",
       "  7.3482175,\n",
       "  7.3427687,\n",
       "  7.3330956,\n",
       "  7.346434,\n",
       "  7.0895114,\n",
       "  7.578004,\n",
       "  7.3843427,\n",
       "  7.1957912,\n",
       "  7.134945,\n",
       "  7.354429,\n",
       "  7.4433136,\n",
       "  7.2567825,\n",
       "  7.258936,\n",
       "  7.141107,\n",
       "  7.1902285,\n",
       "  7.2241783,\n",
       "  7.211326,\n",
       "  7.202578,\n",
       "  7.1687455,\n",
       "  7.157774,\n",
       "  7.1944,\n",
       "  7.162781,\n",
       "  7.129906,\n",
       "  7.1836824,\n",
       "  7.1856356,\n",
       "  7.1611576,\n",
       "  7.1928473,\n",
       "  7.1705394,\n",
       "  7.1291575,\n",
       "  7.1730547,\n",
       "  7.185772,\n",
       "  7.148249,\n",
       "  7.1454015,\n",
       "  7.1091056,\n",
       "  7.0933084,\n",
       "  7.098727,\n",
       "  7.0744114,\n",
       "  7.062542,\n",
       "  7.0859265,\n",
       "  7.071266,\n",
       "  7.081049,\n",
       "  7.111707,\n",
       "  7.1000805,\n",
       "  7.080279,\n",
       "  7.0585127,\n",
       "  7.0431933,\n",
       "  7.0185504,\n",
       "  6.999147,\n",
       "  7.0313354,\n",
       "  7.009589,\n",
       "  6.983249,\n",
       "  6.9807067,\n",
       "  6.969112,\n",
       "  6.96635,\n",
       "  6.94408,\n",
       "  6.9563117,\n",
       "  6.9325147,\n",
       "  6.9513984,\n",
       "  6.9535217,\n",
       "  6.9567103,\n",
       "  6.957809,\n",
       "  6.973209,\n",
       "  6.9722834,\n",
       "  6.9833064,\n",
       "  6.9803395,\n",
       "  6.9696846,\n",
       "  6.982824,\n",
       "  6.9978013,\n",
       "  6.9974613,\n",
       "  6.997345,\n",
       "  7.0060477,\n",
       "  6.997643,\n",
       "  6.9843035,\n",
       "  6.983114,\n",
       "  6.9780564,\n",
       "  6.970564,\n",
       "  6.9610004,\n",
       "  6.972691,\n",
       "  6.971468,\n",
       "  6.9734745,\n",
       "  6.966402,\n",
       "  6.954192,\n",
       "  6.951284,\n",
       "  7.8781977,\n",
       "  7.3688684,\n",
       "  7.247154,\n",
       "  7.0482526,\n",
       "  7.0096083,\n",
       "  6.946148,\n",
       "  7.0254745,\n",
       "  6.9293585,\n",
       "  6.8829494,\n",
       "  6.8326225,\n",
       "  6.8271747,\n",
       "  6.8106003,\n",
       "  6.7695484,\n",
       "  6.8773923,\n",
       "  6.85171,\n",
       "  6.8517113,\n",
       "  6.8467503,\n",
       "  6.842629,\n",
       "  6.822882,\n",
       "  6.8831344,\n",
       "  6.8557973,\n",
       "  6.8545012,\n",
       "  6.8594446,\n",
       "  6.856912,\n",
       "  6.846948,\n",
       "  6.863731,\n",
       "  6.904834,\n",
       "  6.8881674,\n",
       "  6.882395,\n",
       "  6.8742685,\n",
       "  6.842171,\n",
       "  6.8366556,\n",
       "  6.834957,\n",
       "  6.8219967,\n",
       "  6.8162,\n",
       "  6.8547316,\n",
       "  6.8441415,\n",
       "  6.851728,\n",
       "  6.8552294,\n",
       "  6.8763475,\n",
       "  6.8845057,\n",
       "  6.873921,\n",
       "  6.875692,\n",
       "  6.8806868,\n",
       "  6.8764615,\n",
       "  6.8765206,\n",
       "  6.8767033,\n",
       "  6.8693886,\n",
       "  6.8539467,\n",
       "  6.843381,\n",
       "  6.8655505,\n",
       "  6.8591723,\n",
       "  6.8457527,\n",
       "  6.851893,\n",
       "  6.8563356,\n",
       "  6.870904,\n",
       "  6.8643413,\n",
       "  6.859135,\n",
       "  6.859059,\n",
       "  6.83888,\n",
       "  6.8307686,\n",
       "  6.8512697,\n",
       "  6.8482137,\n",
       "  6.857292,\n",
       "  6.8504944,\n",
       "  6.8433423,\n",
       "  6.841116,\n",
       "  6.840611,\n",
       "  6.840073,\n",
       "  6.827429,\n",
       "  6.8318896,\n",
       "  6.818899,\n",
       "  6.833262,\n",
       "  6.8318315,\n",
       "  6.82203,\n",
       "  6.8200545,\n",
       "  6.818321,\n",
       "  6.806543,\n",
       "  6.820226,\n",
       "  6.6811657,\n",
       "  6.9054985,\n",
       "  6.671579,\n",
       "  6.568343,\n",
       "  6.4041967,\n",
       "  6.445816,\n",
       "  6.578767,\n",
       "  6.542075,\n",
       "  6.598715,\n",
       "  6.5216627,\n",
       "  6.518373,\n",
       "  6.466597,\n",
       "  6.5272093,\n",
       "  6.541017,\n",
       "  6.5624423,\n",
       "  6.4937077,\n",
       "  6.5225086,\n",
       "  6.539461,\n",
       "  6.517444,\n",
       "  6.529891,\n",
       "  6.513167,\n",
       "  6.4956465,\n",
       "  6.4901533,\n",
       "  6.4869046,\n",
       "  6.4668536,\n",
       "  6.504615,\n",
       "  6.5040317,\n",
       "  6.5197587,\n",
       "  6.5286574,\n",
       "  6.5092206,\n",
       "  6.5252037,\n",
       "  6.5018454,\n",
       "  6.5583,\n",
       "  6.604255,\n",
       "  6.60938,\n",
       "  6.596975,\n",
       "  6.5877457,\n",
       "  6.5875244,\n",
       "  6.570128,\n",
       "  6.5661573,\n",
       "  6.567436,\n",
       "  6.558946,\n",
       "  6.584566,\n",
       "  6.58398,\n",
       "  6.580165,\n",
       "  6.589983,\n",
       "  6.620844,\n",
       "  6.6080623,\n",
       "  6.5960345,\n",
       "  6.5825324,\n",
       "  6.587346,\n",
       "  6.5813293,\n",
       "  6.580979,\n",
       "  6.586068,\n",
       "  6.615359,\n",
       "  6.6381683,\n",
       "  6.6357026,\n",
       "  6.62825,\n",
       "  6.6437554,\n",
       "  6.6615925,\n",
       "  6.646429,\n",
       "  6.6399245,\n",
       "  6.627379,\n",
       "  6.627554,\n",
       "  6.642776,\n",
       "  6.6693068,\n",
       "  6.671592,\n",
       "  6.683377,\n",
       "  6.6945367,\n",
       "  6.6831017,\n",
       "  6.6747246,\n",
       "  6.6755424,\n",
       "  6.695008,\n",
       "  6.6942005,\n",
       "  6.681226,\n",
       "  6.675978,\n",
       "  6.69217,\n",
       "  6.6971164,\n",
       "  6.7196493,\n",
       "  5.6650724,\n",
       "  5.767799,\n",
       "  5.9314895,\n",
       "  6.4232945,\n",
       "  6.4014993,\n",
       "  6.2892118,\n",
       "  6.337897,\n",
       "  6.520108,\n",
       "  6.53342,\n",
       "  6.4978266,\n",
       "  6.4858794,\n",
       "  6.427671,\n",
       "  6.437747,\n",
       "  6.4875884,\n",
       "  6.4867654,\n",
       "  6.5006475,\n",
       "  6.463205,\n",
       "  6.458031,\n",
       "  6.4739194,\n",
       "  6.4751296,\n",
       "  6.519867,\n",
       "  6.497582,\n",
       "  6.5169506,\n",
       "  6.491586,\n",
       "  6.492287,\n",
       "  6.502212,\n",
       "  6.5062675,\n",
       "  6.479785,\n",
       "  6.468354,\n",
       "  6.496279,\n",
       "  6.4939857,\n",
       "  6.4860535,\n",
       "  6.5096126,\n",
       "  6.5076303,\n",
       "  6.4981275,\n",
       "  6.4980626,\n",
       "  6.4966025,\n",
       "  6.501953,\n",
       "  6.500787,\n",
       "  6.506526,\n",
       "  6.515109,\n",
       "  6.492713,\n",
       "  6.4867597,\n",
       "  6.520624,\n",
       "  6.514457,\n",
       "  6.5019956,\n",
       "  6.4933143,\n",
       "  6.5081444,\n",
       "  6.4966435,\n",
       "  6.508259,\n",
       "  6.499602,\n",
       "  6.499148,\n",
       "  6.480904,\n",
       "  6.4959354,\n",
       "  6.4883976,\n",
       "  6.491745,\n",
       "  6.501137,\n",
       "  6.4946938,\n",
       "  6.516996,\n",
       "  6.502459,\n",
       "  6.5021353,\n",
       "  6.49791,\n",
       "  6.4975247,\n",
       "  6.5142026,\n",
       "  6.5122485,\n",
       "  6.5099077,\n",
       "  6.507399,\n",
       "  6.507078,\n",
       "  6.5097027,\n",
       "  6.503292,\n",
       "  6.498561,\n",
       "  6.5127826,\n",
       "  6.504637,\n",
       "  6.504896,\n",
       "  6.507498,\n",
       "  6.503887,\n",
       "  6.510008,\n",
       "  6.496423,\n",
       "  6.4936757,\n",
       "  7.1292553,\n",
       "  6.610548,\n",
       "  6.677196,\n",
       "  6.880928,\n",
       "  6.807706,\n",
       "  6.825367,\n",
       "  6.767253,\n",
       "  6.6765857,\n",
       "  6.7183113,\n",
       "  6.774391,\n",
       "  6.6997423,\n",
       "  6.7050304,\n",
       "  6.715102,\n",
       "  6.63015,\n",
       "  6.6550946,\n",
       "  6.6085243,\n",
       "  6.6422887,\n",
       "  6.636004,\n",
       "  6.66098,\n",
       "  6.6586547,\n",
       "  6.63926,\n",
       "  6.6355147,\n",
       "  6.655124,\n",
       "  6.668218,\n",
       "  6.6872725,\n",
       "  6.6771045,\n",
       "  6.6830015,\n",
       "  6.6958137,\n",
       "  6.6822734,\n",
       "  6.726211,\n",
       "  6.7071896,\n",
       "  6.6954193,\n",
       "  6.6730585,\n",
       "  6.6707406,\n",
       "  6.6578226,\n",
       "  6.663251,\n",
       "  6.6450424,\n",
       "  6.658925,\n",
       "  6.667994,\n",
       "  6.653857,\n",
       "  6.6406846,\n",
       "  6.652273,\n",
       "  6.6306243,\n",
       "  6.641653,\n",
       "  6.6249924,\n",
       "  6.629211,\n",
       "  6.606578,\n",
       "  6.6141963,\n",
       "  6.620805,\n",
       "  6.619055,\n",
       "  6.617945,\n",
       "  6.6150413,\n",
       "  ...],\n",
       " 'mean_val_loss': [4.87926,\n",
       "  4.44965,\n",
       "  4.2872496,\n",
       "  4.0929723,\n",
       "  4.0043945,\n",
       "  3.8754363,\n",
       "  4.001093,\n",
       "  3.784858,\n",
       "  4.2041597,\n",
       "  4.0382977,\n",
       "  3.9466217,\n",
       "  3.749704,\n",
       "  3.8158526,\n",
       "  3.8013763,\n",
       "  3.9095702],\n",
       " 'min_val_loss': 3.749704,\n",
       " 'train_time': 7941.052913427353,\n",
       " 'test_loss': [tensor(3.6462),\n",
       "  tensor(4.0916),\n",
       "  tensor(3.9566),\n",
       "  tensor(3.9063),\n",
       "  tensor(3.8079)],\n",
       " 'mean_test_loss': 3.8817272}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_filename = 'FF_sep_2048_1layer_9neg_rad2_ReLU_stats.pkl'\n",
    "stats = torch.load(os.getcwd()+'/checkpoints/{}'.format(stats_filename),\n",
    "          map_location=torch.device('cpu'))\n",
    "stats  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filename = 'FF_sep_2048_1layer_9neg_rad2_ReLU_checkpoint_0011.pth.tar'\n",
    "checkpoint = torch.load(os.getcwd()+'/checkpoints/{}'.format(checkpoint_filename),\n",
    "          map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainargs = {'model': 'FF_sep',\n",
    "  'hidden_sizes': [256],\n",
    "  'output_size': 1,\n",
    "  'dropout': 0.5,\n",
    "  'batch_size': 512,\n",
    "  'activation': 'ReLU',\n",
    "  'optimizer': torch.optim.Adam,\n",
    "  'learning_rate': 1e-05,\n",
    "  'epochs': 50,\n",
    "  'early_stop': True,\n",
    "  'min_delta': 1e-05,\n",
    "  'patience': 3,\n",
    "  'checkpoint': True,\n",
    "  'model_seed': 1337,\n",
    "  'random_seed': 0,\n",
    "  'rctfp_size': 2048,\n",
    "  'prodfp_size': 2048,\n",
    "  'fp_radius': 2,\n",
    "  'fp_type': 'sep',\n",
    "  'num_neg': 9,\n",
    "    'path_to_pickle': os.getcwd()+'/clean_rxn_50k_nomap_noreagent.pickle', \n",
    "    'checkpoint_path': os.getcwd()+'/checkpoints/',\n",
    " 'expt_name': '2048_1layer_1neg_rad2_ReLU',\n",
    "     'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FF_ebm(trainargs)\n",
    "optimizer = trainargs['optimizer'](model.parameters(), lr=trainargs['learning_rate'])\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "run = Run(model, trainargs, optimizer, load_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:25<00:00, 17.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11543838625923707"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = ReactionDataset(trainargs['path_to_pickle'], 'test', trainargs)\n",
    "test_loader = DataLoader(test_dataset, 2 * trainargs['batch_size'], shuffle=False)\n",
    "\n",
    "run.get_topk_acc(test_loader, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:45<00:00, 21.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19412822049131218"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_topk_acc(test_loader, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load best model (87% top-1 acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can just load trainargs from checkpoint['stats']['trainargs'] also, \n",
    "# but most likely we want to change epochs or learning rate, so redefining it this way is better \n",
    "\n",
    "trainargs = {\n",
    "    'model': 'FF_diff', # must change both model & fp_type \n",
    "    'hidden_sizes': [256],  \n",
    "    'output_size': 1,\n",
    "    'dropout': 0.3, # adapted from Reaction Condition Recommender   \n",
    "    \n",
    "    'batch_size': 512,\n",
    "    'activation': 'ReLU', # trying ELU for its differentiability everywhere (vs ReLU which is not differentiable at x=0)\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'learning_rate': 5e-5, # to try: integrate w/ fast.ai lr_finder & lr_schedulers \n",
    "    'epochs': 50,\n",
    "    'early_stop': True,\n",
    "    'min_delta': 1e-5, \n",
    "    'patience': 3,\n",
    "\n",
    "    'checkpoint': True,\n",
    "    'model_seed': 1337,\n",
    "    'random_seed': 0, # affects neg rxn sampling since it is random\n",
    "    \n",
    "    'rctfp_size': 4096, # if fp_type == 'diff', ensure that both rctfp_size & prodfp_size are identical!\n",
    "    'prodfp_size': 4096,\n",
    "    'fp_radius': 3,\n",
    "    'fp_type': 'diff',\n",
    "    \n",
    "    'num_neg': 5, # to be tuned, 9 seems to be superior to 5 (overfitting occured quickly)\n",
    "    \n",
    "    'path_to_pickle': os.getcwd()+'/clean_rxn_50k_nomap_noreagent.pickle', \n",
    "    'checkpoint_path': os.getcwd()+'/checkpoints/',\n",
    "    'expt_name': 'test',\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model', 'state_dict', 'optimizer', 'stats'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(os.getcwd()+'/checkpoints/FF_diff_DIFF_4096_1layer_5neg_rad3_ReLU_checkpoint_0049.pth.tar',\n",
    "          map_location=torch.device('cpu'))\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trainargs': {'model': 'FF_diff',\n",
       "  'hidden_sizes': [256],\n",
       "  'output_size': 1,\n",
       "  'dropout': 0.5,\n",
       "  'batch_size': 512,\n",
       "  'activation': 'ReLU',\n",
       "  'optimizer': torch.optim.adam.Adam,\n",
       "  'learning_rate': 5e-05,\n",
       "  'epochs': 50,\n",
       "  'early_stop': True,\n",
       "  'min_delta': 1e-05,\n",
       "  'patience': 3,\n",
       "  'checkpoint': True,\n",
       "  'model_seed': 1337,\n",
       "  'random_seed': 0,\n",
       "  'rctfp_size': 4096,\n",
       "  'prodfp_size': 4096,\n",
       "  'fp_radius': 3,\n",
       "  'fp_type': 'diff',\n",
       "  'num_neg': 5,\n",
       "  'path_to_pickle': '/content/clean_rxn_50k_nomap_noreagent.pickle',\n",
       "  'checkpoint_path': '/content/gdrive/My Drive/rxn_ebm/',\n",
       "  'expt_name': 'DIFF_4096_1layer_5neg_rad3_ReLU'},\n",
       " 'mean_train_loss': [5.541507,\n",
       "  5.738795,\n",
       "  5.986399,\n",
       "  6.1413307,\n",
       "  6.0781894,\n",
       "  6.124495,\n",
       "  6.146295,\n",
       "  6.0141373,\n",
       "  5.8681197,\n",
       "  5.8325505,\n",
       "  5.797415,\n",
       "  5.7253375,\n",
       "  5.736352,\n",
       "  5.6945977,\n",
       "  5.7136016,\n",
       "  5.6857224,\n",
       "  5.7454643,\n",
       "  5.759552,\n",
       "  5.749079,\n",
       "  5.777142,\n",
       "  5.755688,\n",
       "  5.7428727,\n",
       "  5.725483,\n",
       "  5.7730203,\n",
       "  5.8116364,\n",
       "  5.8061557,\n",
       "  5.791269,\n",
       "  5.785485,\n",
       "  5.77496,\n",
       "  5.7594767,\n",
       "  5.7567453,\n",
       "  5.7538157,\n",
       "  5.7545,\n",
       "  5.7605352,\n",
       "  5.753414,\n",
       "  5.722416,\n",
       "  5.7014666,\n",
       "  5.6752734,\n",
       "  5.650992,\n",
       "  5.6279664,\n",
       "  5.609492,\n",
       "  5.582098,\n",
       "  5.554753,\n",
       "  5.5393505,\n",
       "  5.5276346,\n",
       "  5.511844,\n",
       "  5.5056777,\n",
       "  5.4843583,\n",
       "  5.4647264,\n",
       "  5.4460683,\n",
       "  5.4362745,\n",
       "  5.4107413,\n",
       "  5.385703,\n",
       "  5.3649344,\n",
       "  5.3431406,\n",
       "  5.3315406,\n",
       "  5.3144126,\n",
       "  5.3040223,\n",
       "  5.2891192,\n",
       "  5.2681303,\n",
       "  5.2468348,\n",
       "  5.2301955,\n",
       "  5.214103,\n",
       "  5.209412,\n",
       "  5.196608,\n",
       "  5.18475,\n",
       "  5.171784,\n",
       "  5.166296,\n",
       "  5.1668324,\n",
       "  5.1569343,\n",
       "  5.148877,\n",
       "  5.1390533,\n",
       "  5.1291447,\n",
       "  5.1206374,\n",
       "  5.1092563,\n",
       "  5.0978127,\n",
       "  5.0832157,\n",
       "  5.0651717,\n",
       "  5.0624647,\n",
       "  4.2084355,\n",
       "  4.074203,\n",
       "  4.0748677,\n",
       "  4.1664653,\n",
       "  4.212293,\n",
       "  4.2592187,\n",
       "  4.288237,\n",
       "  4.282127,\n",
       "  4.3009415,\n",
       "  4.3047247,\n",
       "  4.263264,\n",
       "  4.2109523,\n",
       "  4.1988673,\n",
       "  4.1800103,\n",
       "  4.168622,\n",
       "  4.1467266,\n",
       "  4.1609,\n",
       "  4.1823397,\n",
       "  4.1630387,\n",
       "  4.1699276,\n",
       "  4.1707163,\n",
       "  4.160845,\n",
       "  4.1772156,\n",
       "  4.220785,\n",
       "  4.2289886,\n",
       "  4.1820803,\n",
       "  4.1380515,\n",
       "  4.0921793,\n",
       "  4.124147,\n",
       "  4.124549,\n",
       "  4.0918694,\n",
       "  4.085557,\n",
       "  4.0952415,\n",
       "  4.083901,\n",
       "  4.059385,\n",
       "  4.040595,\n",
       "  4.020668,\n",
       "  4.0027204,\n",
       "  4.008773,\n",
       "  4.009777,\n",
       "  4.020533,\n",
       "  4.050241,\n",
       "  4.0281334,\n",
       "  4.0106263,\n",
       "  4.021562,\n",
       "  4.0081606,\n",
       "  3.9974775,\n",
       "  4.003513,\n",
       "  4.0245376,\n",
       "  4.0271125,\n",
       "  4.026113,\n",
       "  4.038135,\n",
       "  4.051051,\n",
       "  4.0576167,\n",
       "  4.040488,\n",
       "  4.0223475,\n",
       "  4.0214195,\n",
       "  4.029604,\n",
       "  4.018534,\n",
       "  4.0239367,\n",
       "  4.018731,\n",
       "  3.9977646,\n",
       "  3.978038,\n",
       "  3.9625757,\n",
       "  3.970537,\n",
       "  3.9811332,\n",
       "  3.9874399,\n",
       "  3.9990225,\n",
       "  4.0173297,\n",
       "  4.035012,\n",
       "  4.0436163,\n",
       "  4.0548334,\n",
       "  4.063863,\n",
       "  4.0633326,\n",
       "  4.0532947,\n",
       "  4.04182,\n",
       "  4.0437775,\n",
       "  4.0462546,\n",
       "  4.034854,\n",
       "  3.9343114,\n",
       "  3.8226159,\n",
       "  4.164717,\n",
       "  4.140777,\n",
       "  4.25567,\n",
       "  4.282885,\n",
       "  4.2225914,\n",
       "  4.1828117,\n",
       "  4.1551595,\n",
       "  4.1024256,\n",
       "  4.0346165,\n",
       "  3.9828942,\n",
       "  3.922662,\n",
       "  3.873224,\n",
       "  3.8478355,\n",
       "  3.845318,\n",
       "  3.8641236,\n",
       "  3.8680217,\n",
       "  3.8601704,\n",
       "  3.8565464,\n",
       "  3.8564885,\n",
       "  3.8520346,\n",
       "  3.84996,\n",
       "  3.8475058,\n",
       "  3.8460245,\n",
       "  3.8520002,\n",
       "  3.8658788,\n",
       "  3.8829043,\n",
       "  3.9029598,\n",
       "  3.895507,\n",
       "  3.8769217,\n",
       "  3.8637376,\n",
       "  3.8636568,\n",
       "  3.8385575,\n",
       "  3.798694,\n",
       "  3.7945192,\n",
       "  3.7735405,\n",
       "  3.757515,\n",
       "  3.7445812,\n",
       "  3.736462,\n",
       "  3.7413595,\n",
       "  3.7335172,\n",
       "  3.7132752,\n",
       "  3.7060544,\n",
       "  3.6808734,\n",
       "  3.6773767,\n",
       "  3.6812675,\n",
       "  3.686591,\n",
       "  3.6968937,\n",
       "  3.709874,\n",
       "  3.7234988,\n",
       "  3.7441227,\n",
       "  3.755771,\n",
       "  3.770178,\n",
       "  3.7870784,\n",
       "  3.8038876,\n",
       "  3.8138392,\n",
       "  3.7869473,\n",
       "  3.7661963,\n",
       "  3.7654517,\n",
       "  3.7475035,\n",
       "  3.741551,\n",
       "  3.7363496,\n",
       "  3.7285357,\n",
       "  3.7210913,\n",
       "  3.6995678,\n",
       "  3.682974,\n",
       "  3.6593277,\n",
       "  3.6422827,\n",
       "  3.636742,\n",
       "  3.6300998,\n",
       "  3.6494412,\n",
       "  3.6606023,\n",
       "  3.6645656,\n",
       "  3.6544616,\n",
       "  3.6408226,\n",
       "  3.6318202,\n",
       "  3.623261,\n",
       "  3.6158137,\n",
       "  3.3847694,\n",
       "  3.6815448,\n",
       "  3.8891714,\n",
       "  4.0059385,\n",
       "  3.996078,\n",
       "  4.026999,\n",
       "  3.9702415,\n",
       "  3.9549463,\n",
       "  3.9120598,\n",
       "  3.841745,\n",
       "  3.8156388,\n",
       "  3.7990634,\n",
       "  3.8053389,\n",
       "  3.7860591,\n",
       "  3.769841,\n",
       "  3.7689753,\n",
       "  3.7558153,\n",
       "  3.7068033,\n",
       "  3.6650944,\n",
       "  3.6402225,\n",
       "  3.622766,\n",
       "  3.6228013,\n",
       "  3.6191807,\n",
       "  3.6122983,\n",
       "  3.6212914,\n",
       "  3.6199346,\n",
       "  3.6187537,\n",
       "  3.6229293,\n",
       "  3.627111,\n",
       "  3.620483,\n",
       "  3.618257,\n",
       "  3.592328,\n",
       "  3.5915306,\n",
       "  3.5909324,\n",
       "  3.5656185,\n",
       "  3.5546377,\n",
       "  3.550576,\n",
       "  3.5555618,\n",
       "  3.5650008,\n",
       "  3.563691,\n",
       "  3.5616474,\n",
       "  3.542419,\n",
       "  3.5430946,\n",
       "  3.533651,\n",
       "  3.530717,\n",
       "  3.5247388,\n",
       "  3.5253797,\n",
       "  3.5207412,\n",
       "  3.5093718,\n",
       "  3.5045388,\n",
       "  3.499196,\n",
       "  3.4972126,\n",
       "  3.4979565,\n",
       "  3.495143,\n",
       "  3.4938123,\n",
       "  3.4897597,\n",
       "  3.4868577,\n",
       "  3.479813,\n",
       "  3.47627,\n",
       "  3.467478,\n",
       "  3.4628458,\n",
       "  3.4624019,\n",
       "  3.462304,\n",
       "  3.4542718,\n",
       "  3.4504528,\n",
       "  3.4458582,\n",
       "  3.4432707,\n",
       "  3.4500294,\n",
       "  3.4516935,\n",
       "  3.4476376,\n",
       "  3.438802,\n",
       "  3.4306993,\n",
       "  3.425492,\n",
       "  3.415661,\n",
       "  3.409459,\n",
       "  3.4092946,\n",
       "  3.407609,\n",
       "  3.4014506,\n",
       "  3.3922896,\n",
       "  3.12193,\n",
       "  3.368976,\n",
       "  3.512973,\n",
       "  3.333821,\n",
       "  3.2690454,\n",
       "  3.3063095,\n",
       "  3.2333589,\n",
       "  3.1505642,\n",
       "  3.11449,\n",
       "  3.0817697,\n",
       "  3.0762808,\n",
       "  3.1186163,\n",
       "  3.143288,\n",
       "  3.1951382,\n",
       "  3.223741,\n",
       "  3.243034,\n",
       "  3.2683764,\n",
       "  3.2469196,\n",
       "  3.2619588,\n",
       "  3.2465281,\n",
       "  3.2678654,\n",
       "  3.271798,\n",
       "  3.2860649,\n",
       "  3.2960768,\n",
       "  3.3072,\n",
       "  3.3179588,\n",
       "  3.3198612,\n",
       "  3.3388004,\n",
       "  3.350082,\n",
       "  3.3575282,\n",
       "  3.3798342,\n",
       "  3.3957782,\n",
       "  3.4073863,\n",
       "  3.3920846,\n",
       "  3.3834107,\n",
       "  3.3689346,\n",
       "  3.3637538,\n",
       "  3.3619678,\n",
       "  3.3635805,\n",
       "  3.3660622,\n",
       "  3.348189,\n",
       "  3.3459537,\n",
       "  3.3322923,\n",
       "  3.31754,\n",
       "  3.3100939,\n",
       "  3.300171,\n",
       "  3.2909997,\n",
       "  3.284048,\n",
       "  3.2752562,\n",
       "  3.2666132,\n",
       "  3.256756,\n",
       "  3.2479978,\n",
       "  3.2441542,\n",
       "  3.2395177,\n",
       "  3.2325842,\n",
       "  3.2445378,\n",
       "  3.2444298,\n",
       "  3.2472944,\n",
       "  3.241245,\n",
       "  3.236762,\n",
       "  3.224791,\n",
       "  3.2198133,\n",
       "  3.209393,\n",
       "  3.2028131,\n",
       "  3.1960566,\n",
       "  3.1892002,\n",
       "  3.1864436,\n",
       "  3.190108,\n",
       "  3.1953645,\n",
       "  3.1891866,\n",
       "  3.1844757,\n",
       "  3.1801822,\n",
       "  3.1783376,\n",
       "  3.1875196,\n",
       "  3.1814692,\n",
       "  3.1726277,\n",
       "  3.1666048,\n",
       "  3.1641324,\n",
       "  3.157205,\n",
       "  2.6014636,\n",
       "  2.6098957,\n",
       "  2.6114628,\n",
       "  2.637062,\n",
       "  2.5838673,\n",
       "  2.5955276,\n",
       "  2.5856125,\n",
       "  2.580483,\n",
       "  2.5651653,\n",
       "  2.5618834,\n",
       "  2.557016,\n",
       "  2.55602,\n",
       "  2.5454595,\n",
       "  2.530953,\n",
       "  2.5263991,\n",
       "  2.5229337,\n",
       "  2.5610409,\n",
       "  2.5504155,\n",
       "  2.583279,\n",
       "  2.5864754,\n",
       "  2.5784533,\n",
       "  2.5825365,\n",
       "  2.5703633,\n",
       "  2.565219,\n",
       "  2.5575504,\n",
       "  2.5588884,\n",
       "  2.5480917,\n",
       "  2.5524068,\n",
       "  2.544072,\n",
       "  2.5377238,\n",
       "  2.5415473,\n",
       "  2.5414,\n",
       "  2.5326161,\n",
       "  2.5296655,\n",
       "  2.5209856,\n",
       "  2.5195177,\n",
       "  2.5162318,\n",
       "  2.5358088,\n",
       "  2.5266838,\n",
       "  2.5240703,\n",
       "  2.5163918,\n",
       "  2.5220788,\n",
       "  2.5200949,\n",
       "  2.5192602,\n",
       "  2.5217953,\n",
       "  2.5121586,\n",
       "  2.5069034,\n",
       "  2.5016198,\n",
       "  2.4978645,\n",
       "  2.495759,\n",
       "  2.496496,\n",
       "  2.5082397,\n",
       "  2.5135596,\n",
       "  2.5118506,\n",
       "  2.5110233,\n",
       "  2.5108027,\n",
       "  2.520238,\n",
       "  2.5270858,\n",
       "  2.537913,\n",
       "  2.54697,\n",
       "  2.54623,\n",
       "  2.5433886,\n",
       "  2.5432327,\n",
       "  2.550486,\n",
       "  2.5484846,\n",
       "  2.5557964,\n",
       "  2.5688918,\n",
       "  2.572124,\n",
       "  2.5859172,\n",
       "  2.5878057,\n",
       "  2.5861852,\n",
       "  2.58817,\n",
       "  2.5906312,\n",
       "  2.594709,\n",
       "  2.5906565,\n",
       "  2.5942974,\n",
       "  2.6139917,\n",
       "  2.6288366,\n",
       "  2.6529944,\n",
       "  2.3540897,\n",
       "  2.3026283,\n",
       "  2.3063111,\n",
       "  2.3692656,\n",
       "  2.4639082,\n",
       "  2.48064,\n",
       "  2.4963584,\n",
       "  2.5410123,\n",
       "  2.5209892,\n",
       "  2.542685,\n",
       "  2.550528,\n",
       "  2.5615952,\n",
       "  2.590656,\n",
       "  2.6483688,\n",
       "  2.713689,\n",
       "  2.7779994,\n",
       "  2.8184876,\n",
       "  2.8238873,\n",
       "  2.8369045,\n",
       "  2.8607802,\n",
       "  2.8745084,\n",
       "  2.889316,\n",
       "  2.9166472,\n",
       "  2.8951685,\n",
       "  2.8714426,\n",
       "  2.8470829,\n",
       "  2.8157294,\n",
       "  2.8133545,\n",
       "  2.8256464,\n",
       "  2.8304102,\n",
       "  2.8203692,\n",
       "  2.8152041,\n",
       "  2.8131044,\n",
       "  2.8122292,\n",
       "  2.8136246,\n",
       "  2.7975888,\n",
       "  2.784429,\n",
       "  2.7722328,\n",
       "  2.7622263,\n",
       "  2.745259,\n",
       "  2.7319465,\n",
       "  2.7210646,\n",
       "  2.7087939,\n",
       "  2.696674,\n",
       "  2.6880167,\n",
       "  2.6804397,\n",
       "  2.6725857,\n",
       "  2.663854,\n",
       "  2.6569133,\n",
       "  2.6497629,\n",
       "  2.642967,\n",
       "  2.6337976,\n",
       "  2.6272588,\n",
       "  2.623319,\n",
       "  2.6202137,\n",
       "  2.614778,\n",
       "  2.614032,\n",
       "  2.6071184,\n",
       "  2.6022859,\n",
       "  2.5949945,\n",
       "  2.5897195,\n",
       "  2.5827508,\n",
       "  2.578057,\n",
       "  2.5739079,\n",
       "  2.5709834,\n",
       "  2.5710356,\n",
       "  2.5693603,\n",
       "  2.5645127,\n",
       "  2.5582082,\n",
       "  2.5537539,\n",
       "  2.5542722,\n",
       "  2.5547075,\n",
       "  2.5531042,\n",
       "  2.5511787,\n",
       "  2.548023,\n",
       "  2.5418832,\n",
       "  2.5359237,\n",
       "  2.5304186,\n",
       "  2.5313196,\n",
       "  2.1810958,\n",
       "  2.3106456,\n",
       "  2.3209386,\n",
       "  2.419074,\n",
       "  2.4102597,\n",
       "  2.3920622,\n",
       "  2.3626559,\n",
       "  2.320765,\n",
       "  2.3149023,\n",
       "  2.3010695,\n",
       "  2.2869713,\n",
       "  2.2989974,\n",
       "  2.301462,\n",
       "  2.2873356,\n",
       "  2.3158302,\n",
       "  2.2988107,\n",
       "  2.2933412,\n",
       "  2.283773,\n",
       "  2.2675219,\n",
       "  2.2646449,\n",
       "  2.273411,\n",
       "  2.269375,\n",
       "  2.268824,\n",
       "  2.2701519,\n",
       "  2.2651844,\n",
       "  2.2638817,\n",
       "  2.267731,\n",
       "  2.26991,\n",
       "  2.2774003,\n",
       "  2.29407,\n",
       "  2.2949326,\n",
       "  2.2974072,\n",
       "  2.295894,\n",
       "  2.2980175,\n",
       "  2.2958844,\n",
       "  2.2903926,\n",
       "  2.2953484,\n",
       "  2.3091736,\n",
       "  2.310041,\n",
       "  2.3069272,\n",
       "  2.3139555,\n",
       "  2.3201025,\n",
       "  2.3227425,\n",
       "  2.3220327,\n",
       "  2.3247375,\n",
       "  2.3257873,\n",
       "  2.3244278,\n",
       "  2.3227055,\n",
       "  2.318555,\n",
       "  2.3122628,\n",
       "  2.3071246,\n",
       "  2.30205,\n",
       "  2.295867,\n",
       "  2.2917416,\n",
       "  2.287984,\n",
       "  2.2890594,\n",
       "  2.2832377,\n",
       "  2.2782435,\n",
       "  2.275155,\n",
       "  2.271609,\n",
       "  2.2670302,\n",
       "  2.2650583,\n",
       "  2.2653496,\n",
       "  2.26931,\n",
       "  2.2715921,\n",
       "  2.2687187,\n",
       "  2.2643037,\n",
       "  2.2614174,\n",
       "  2.2599006,\n",
       "  2.2598703,\n",
       "  2.2568648,\n",
       "  2.2534862,\n",
       "  2.2506275,\n",
       "  2.2456408,\n",
       "  2.24172,\n",
       "  2.241823,\n",
       "  2.2418363,\n",
       "  2.2389655,\n",
       "  2.2356968,\n",
       "  1.9536662,\n",
       "  1.9857433,\n",
       "  1.9793625,\n",
       "  1.980787,\n",
       "  1.974913,\n",
       "  1.9950752,\n",
       "  2.0055258,\n",
       "  2.004552,\n",
       "  2.0074553,\n",
       "  2.0287385,\n",
       "  2.0296402,\n",
       "  2.0787156,\n",
       "  2.116427,\n",
       "  2.151964,\n",
       "  2.151872,\n",
       "  2.1546335,\n",
       "  2.162752,\n",
       "  2.1773326,\n",
       "  2.1937497,\n",
       "  2.220467,\n",
       "  2.2359273,\n",
       "  2.2474668,\n",
       "  2.285963,\n",
       "  2.3101203,\n",
       "  2.335902,\n",
       "  2.347466,\n",
       "  2.368528,\n",
       "  2.3759935,\n",
       "  2.382708,\n",
       "  2.369529,\n",
       "  2.3550532,\n",
       "  2.3432837,\n",
       "  2.3332896,\n",
       "  2.321148,\n",
       "  2.3095844,\n",
       "  2.2982159,\n",
       "  2.290977,\n",
       "  2.2814682,\n",
       "  2.272215,\n",
       "  2.2655349,\n",
       "  2.256936,\n",
       "  2.2508106,\n",
       "  2.2410545,\n",
       "  2.2369661,\n",
       "  2.2306647,\n",
       "  2.2266448,\n",
       "  2.2322416,\n",
       "  2.2382984,\n",
       "  2.247022,\n",
       "  2.257742,\n",
       "  2.25401,\n",
       "  2.2581012,\n",
       "  2.2547014,\n",
       "  2.2503176,\n",
       "  2.255441,\n",
       "  2.2496834,\n",
       "  2.2476206,\n",
       "  2.2420287,\n",
       "  2.2473776,\n",
       "  2.2408187,\n",
       "  2.2439966,\n",
       "  2.251637,\n",
       "  2.2531111,\n",
       "  2.2603211,\n",
       "  2.2581468,\n",
       "  2.2610385,\n",
       "  2.256246,\n",
       "  2.250006,\n",
       "  2.24518,\n",
       "  2.2501972,\n",
       "  2.2533758,\n",
       "  2.2581613,\n",
       "  2.2541199,\n",
       "  2.2503967,\n",
       "  2.2483118,\n",
       "  2.2433033,\n",
       "  2.2407725,\n",
       "  2.237868,\n",
       "  2.2362018,\n",
       "  2.2551937,\n",
       "  2.2599096,\n",
       "  2.230369,\n",
       "  2.2223468,\n",
       "  2.17416,\n",
       "  2.1410198,\n",
       "  2.111625,\n",
       "  2.115036,\n",
       "  2.106006,\n",
       "  2.1031692,\n",
       "  2.0885751,\n",
       "  2.0771558,\n",
       "  2.074347,\n",
       "  2.0564816,\n",
       "  2.0390112,\n",
       "  2.0222087,\n",
       "  2.0074599,\n",
       "  1.9954821,\n",
       "  1.9898465,\n",
       "  1.9902891,\n",
       "  1.9885643,\n",
       "  1.9878455,\n",
       "  1.9768122,\n",
       "  1.964161,\n",
       "  1.958058,\n",
       "  1.9542029,\n",
       "  1.9464487,\n",
       "  1.9409659,\n",
       "  1.9374896,\n",
       "  1.9373361,\n",
       "  1.931331,\n",
       "  1.9282738,\n",
       "  1.9238962,\n",
       "  1.920212,\n",
       "  1.9160165,\n",
       "  1.9145703,\n",
       "  1.909542,\n",
       "  1.9046761,\n",
       "  1.9006754,\n",
       "  1.9000963,\n",
       "  1.8980618,\n",
       "  1.8990682,\n",
       "  1.9022042,\n",
       "  1.9026753,\n",
       "  1.9026686,\n",
       "  1.8989143,\n",
       "  1.8961196,\n",
       "  1.8946177,\n",
       "  1.8900908,\n",
       "  1.8860283,\n",
       "  1.8844559,\n",
       "  1.8839061,\n",
       "  1.8849928,\n",
       "  1.8832542,\n",
       "  1.8832719,\n",
       "  1.8812768,\n",
       "  1.8789846,\n",
       "  1.879071,\n",
       "  1.8805954,\n",
       "  1.8816241,\n",
       "  1.8844392,\n",
       "  1.882534,\n",
       "  1.8853515,\n",
       "  1.8838117,\n",
       "  1.8840568,\n",
       "  1.8856648,\n",
       "  1.8854656,\n",
       "  1.8880671,\n",
       "  1.8892121,\n",
       "  1.8853664,\n",
       "  1.8865896,\n",
       "  1.8873526,\n",
       "  1.8857443,\n",
       "  1.8837101,\n",
       "  1.8832642,\n",
       "  1.8845199,\n",
       "  1.8834405,\n",
       "  1.8848081,\n",
       "  1.8861395,\n",
       "  2.5551558,\n",
       "  2.4971726,\n",
       "  2.3546176,\n",
       "  2.2531843,\n",
       "  2.1586611,\n",
       "  2.1076205,\n",
       "  2.0579648,\n",
       "  2.016994,\n",
       "  1.9935368,\n",
       "  1.9785011,\n",
       "  1.9919502,\n",
       "  2.0067892,\n",
       "  1.9914291,\n",
       "  1.992979,\n",
       "  1.982499,\n",
       "  1.9763013,\n",
       "  1.9700907,\n",
       "  1.9604914,\n",
       "  1.9557692,\n",
       "  1.9540129,\n",
       "  1.9431014,\n",
       "  1.9317459,\n",
       "  1.9249141,\n",
       "  1.9197054,\n",
       "  1.9191366,\n",
       "  1.9157293,\n",
       "  1.9128443,\n",
       "  1.9071586,\n",
       "  1.9039428,\n",
       "  1.9026654,\n",
       "  1.9017994,\n",
       "  1.9026489,\n",
       "  1.9024091,\n",
       "  1.9034868,\n",
       "  1.9034888,\n",
       "  1.9031146,\n",
       "  1.8987057,\n",
       "  1.8959389,\n",
       "  1.8932427,\n",
       "  1.8888264,\n",
       "  1.8850422,\n",
       "  1.8815264,\n",
       "  1.8774669,\n",
       "  1.8732651,\n",
       "  1.8688799,\n",
       "  1.8660982,\n",
       "  1.8662406,\n",
       "  1.8664279,\n",
       "  1.8650882,\n",
       "  1.8638061,\n",
       "  1.8633862,\n",
       "  1.8607305,\n",
       "  1.8592409,\n",
       "  1.8608521,\n",
       "  1.8603467,\n",
       "  1.8563385,\n",
       "  1.8543239,\n",
       "  1.8543915,\n",
       "  1.854449,\n",
       "  1.8549085,\n",
       "  1.8538319,\n",
       "  1.8536596,\n",
       "  1.8539664,\n",
       "  1.8537254,\n",
       "  1.8532224,\n",
       "  1.8515946,\n",
       "  1.8519095,\n",
       "  1.8514464,\n",
       "  1.849868,\n",
       "  1.8479806,\n",
       "  1.8471171,\n",
       "  1.8459721,\n",
       "  1.8441049,\n",
       "  1.8416805,\n",
       "  1.8400543,\n",
       "  1.8415257,\n",
       "  1.8407934,\n",
       "  1.8408898,\n",
       "  1.8439294,\n",
       "  1.694457,\n",
       "  1.7061396,\n",
       "  1.7083416,\n",
       "  1.721056,\n",
       "  1.7551129,\n",
       "  1.7616404,\n",
       "  1.7584455,\n",
       "  1.7721531,\n",
       "  1.7685386,\n",
       "  1.7714198,\n",
       "  1.7749854,\n",
       "  1.7770478,\n",
       "  1.7821889,\n",
       "  1.7854322,\n",
       "  1.7875934,\n",
       "  1.7895176,\n",
       "  1.7925729,\n",
       "  1.789202,\n",
       "  1.7917111,\n",
       "  1.7956365,\n",
       "  1.7983755,\n",
       "  1.797794,\n",
       "  1.8014404,\n",
       "  1.7989105,\n",
       "  1.795614,\n",
       "  1.7911249,\n",
       "  1.7867296,\n",
       "  1.782315,\n",
       "  1.7801077,\n",
       "  1.7778543,\n",
       "  1.7746903,\n",
       "  1.771245,\n",
       "  1.7711439,\n",
       "  1.7696505,\n",
       "  1.7665919,\n",
       "  1.7658408,\n",
       "  1.7636865,\n",
       "  1.7623785,\n",
       "  1.7616022,\n",
       "  1.7603981,\n",
       "  1.7582129,\n",
       "  1.7558115,\n",
       "  1.75316,\n",
       "  1.7534817,\n",
       "  1.7508969,\n",
       "  1.7500726,\n",
       "  1.7512841,\n",
       "  1.7527155,\n",
       "  1.7521952,\n",
       "  1.7547776,\n",
       "  1.7563668,\n",
       "  1.7565815,\n",
       "  1.7565788,\n",
       "  1.7570021,\n",
       "  1.7560406,\n",
       "  1.7554611,\n",
       "  1.7545913,\n",
       "  1.7529987,\n",
       "  1.754893,\n",
       "  1.7545648,\n",
       "  1.7530004,\n",
       "  1.7524042,\n",
       "  1.7513062,\n",
       "  1.7507038,\n",
       "  1.7502351,\n",
       "  1.7496989,\n",
       "  1.7477945,\n",
       "  1.7478912,\n",
       "  1.7478204,\n",
       "  1.7480043,\n",
       "  1.7476535,\n",
       "  1.7485121,\n",
       "  1.7474792,\n",
       "  1.745382,\n",
       "  1.745873,\n",
       "  1.7451386,\n",
       "  1.7443638,\n",
       "  1.7429894,\n",
       "  1.7415749,\n",
       "  1.8213091,\n",
       "  1.8319538,\n",
       "  1.8359419,\n",
       "  1.8259995,\n",
       "  1.8024242,\n",
       "  1.7756323,\n",
       "  1.7563366,\n",
       "  1.7570518,\n",
       "  1.7627543,\n",
       "  1.7573125,\n",
       "  1.7466302,\n",
       "  1.7382013,\n",
       "  1.7392229,\n",
       "  1.7432067,\n",
       "  1.7443423,\n",
       "  1.749037,\n",
       "  1.7418544,\n",
       "  1.7356681,\n",
       "  1.7364522,\n",
       "  1.731602,\n",
       "  1.7288495,\n",
       "  1.7292752,\n",
       "  1.7329003,\n",
       "  1.736071,\n",
       "  1.7351571,\n",
       "  1.7343725,\n",
       "  1.7322621,\n",
       "  1.7304963,\n",
       "  1.7282081,\n",
       "  1.7262712,\n",
       "  1.726571,\n",
       "  1.7280054,\n",
       "  1.7271624,\n",
       "  1.7310245,\n",
       "  1.730022,\n",
       "  1.7303061,\n",
       "  1.7268757,\n",
       "  1.7279775,\n",
       "  1.7262685,\n",
       "  1.7260987,\n",
       "  1.7292397,\n",
       "  1.730853,\n",
       "  1.7311435,\n",
       "  1.7324947,\n",
       "  1.731889,\n",
       "  1.7301601,\n",
       "  1.7301434,\n",
       "  1.7321835,\n",
       "  1.7319714,\n",
       "  1.7331463,\n",
       "  1.7317468,\n",
       "  1.7317224,\n",
       "  ...],\n",
       " 'mean_val_loss': [1.8534687,\n",
       "  1.7465127,\n",
       "  1.6912658,\n",
       "  1.6318344,\n",
       "  1.5835823,\n",
       "  1.5612835,\n",
       "  1.5579973,\n",
       "  1.5084827,\n",
       "  1.5001184,\n",
       "  1.474925,\n",
       "  1.501848,\n",
       "  1.4876436,\n",
       "  1.4705311,\n",
       "  1.4448718,\n",
       "  1.421491,\n",
       "  1.4025317,\n",
       "  1.3804996,\n",
       "  1.3686845,\n",
       "  1.3511384,\n",
       "  1.3622531,\n",
       "  1.350152,\n",
       "  1.3154898,\n",
       "  1.3273813,\n",
       "  1.3181727,\n",
       "  1.2911432,\n",
       "  1.299202,\n",
       "  1.3016821,\n",
       "  1.2945918,\n",
       "  1.2871071,\n",
       "  1.2615128,\n",
       "  1.2650689,\n",
       "  1.2640438,\n",
       "  1.2575208,\n",
       "  1.2521441,\n",
       "  1.2440487,\n",
       "  1.2360499,\n",
       "  1.2230577,\n",
       "  1.2178936,\n",
       "  1.2315698,\n",
       "  1.2176616,\n",
       "  1.2174599,\n",
       "  1.2073994,\n",
       "  1.2171615,\n",
       "  1.2064724,\n",
       "  1.203182,\n",
       "  1.2085292,\n",
       "  1.2001805,\n",
       "  1.198323,\n",
       "  1.1858318,\n",
       "  1.1981033],\n",
       " 'min_val_loss': 1.1858318,\n",
       " 'train_time': 16319.618515729904,\n",
       " 'test_loss': [tensor(1.1271),\n",
       "  tensor(1.1888),\n",
       "  tensor(1.1488),\n",
       "  tensor(1.1864),\n",
       "  tensor(1.1430)],\n",
       " 'mean_test_loss': 1.1588225}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = torch.load(os.getcwd()+'/checkpoints/FF_diff_DIFF_4096_1layer_5neg_rad3_ReLU_stats.pkl',\n",
    "          map_location=torch.device('cpu'))\n",
    "stats  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FF_ebm(trainargs)\n",
    "optimizer = trainargs['optimizer'](model.parameters(), lr=trainargs['learning_rate'])\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "run = Run(model, trainargs, optimizer, load_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|████████████████▊                                                                   | 1/5 [00:13<00:52, 13.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:25<00:39, 13.09s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:39<00:26, 13.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:53<00:13, 13.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:06<00:00, 13.34s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8765727980826843"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = ReactionDataset(trainargs['path_to_pickle'], 'test', trainargs)\n",
    "test_loader = DataLoader(test_dataset, 2 * trainargs['batch_size'], shuffle=False)\n",
    "\n",
    "run.get_topk_acc(test_loader, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|████████████████▊                                                                   | 1/5 [00:09<00:37,  9.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:18<00:28,  9.37s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:29<00:19,  9.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:40<00:10, 10.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:50<00:00, 10.11s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9358897543439185"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_topk_acc(test_loader, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|████████████████▊                                                                   | 1/5 [00:09<00:37,  9.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:19<00:28,  9.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:30<00:19,  9.99s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:40<00:10, 10.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:50<00:00, 10.14s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9658478130617136"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_topk_acc(test_loader, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1285)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which idx has lowest prob[:, 0] --> most incorrect predictions \n",
    "# but cannot retrieve structures, bcos morganFP loses info about structure \n",
    "# but an idea to explore in future for sure \n",
    "\n",
    "# torch.argmin(torch.topk(energy_scores, 1, dim=1)[0]) --> tensor(1285) \n",
    "# test_dataset.__getitem__(1285)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwqAIqlwBkci"
   },
   "source": [
    "### archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "WkC6sg1w4SiA",
    "outputId": "4b657609-3d97-4544-b7b0-d10a02ddb4e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:10<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_time: 593.6578595638275\n",
      "test_loss: [tensor(2.1238), tensor(1.8693), tensor(1.9509), tensor(1.8589), tensor(2.0245)]\n",
      "mean_test_loss: 1.9654802083969116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats = test(model, stats, trainargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "j6F9aiIp6gCP",
    "outputId": "cea2c57d-0637-465d-e314-4b7052d0f7a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ReactionDataset(trainargs['path_to_pickle'], 'test', trainargs)\n",
    "test_loader = DataLoader(test_dataset, 2 * trainargs['batch_size'], shuffle=False)\n",
    "\n",
    "test_scores = get_scores(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "YcIv0BOU72G-",
    "outputId": "c381fdd8-3151-4c5d-f1f9-7dd57a766dad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 31.7568,   0.6232],\n",
       "        [  0.6059,   1.6602],\n",
       "        [  0.5389,   0.5825],\n",
       "        [  0.6551,   1.1973],\n",
       "        [  0.6551,   0.2540],\n",
       "        [  0.5131,   1.4135],\n",
       "        [  0.7937,   0.3980],\n",
       "        [  1.1794,   0.3391],\n",
       "        [  0.9399,   1.2554],\n",
       "        [  1.1911,   0.8047],\n",
       "        [  0.3444,   1.1524],\n",
       "        [  1.5007,   0.3585],\n",
       "        [  1.1836,   1.3253],\n",
       "        [  1.5233,   1.3346],\n",
       "        [-13.7237,   1.2437],\n",
       "        [  1.3159,   0.5960],\n",
       "        [  1.3634,   0.2379],\n",
       "        [  1.0724,   0.2915],\n",
       "        [  0.3685,   1.1581],\n",
       "        [  1.3275,   1.2685]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xn0K4LBo9BiK",
    "outputId": "55ea9c4f-f935-4305-b064-2d528cbfd263"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'early_stop_epoch': 5,\n",
       " 'mean_test_loss': 1.9654802,\n",
       " 'mean_train_loss': [3.79339,\n",
       "  4.147327,\n",
       "  4.1827908,\n",
       "  4.0512996,\n",
       "  4.0508604,\n",
       "  4.0648336,\n",
       "  4.0370502,\n",
       "  4.0900755,\n",
       "  4.050375,\n",
       "  4.071214,\n",
       "  4.116623,\n",
       "  4.1053796,\n",
       "  4.14862,\n",
       "  4.1839542,\n",
       "  4.147746,\n",
       "  4.131441,\n",
       "  4.1477966,\n",
       "  4.174607,\n",
       "  4.1787944,\n",
       "  4.1921945,\n",
       "  4.1756163,\n",
       "  4.181274,\n",
       "  4.191578,\n",
       "  4.2076545,\n",
       "  4.203568,\n",
       "  4.2123814,\n",
       "  4.213587,\n",
       "  4.215596,\n",
       "  4.2241488,\n",
       "  4.2082777,\n",
       "  4.2193255,\n",
       "  4.2122393,\n",
       "  4.200224,\n",
       "  4.1940913,\n",
       "  4.19744,\n",
       "  4.1791553,\n",
       "  4.1880274,\n",
       "  4.198185,\n",
       "  4.2058096,\n",
       "  4.218196,\n",
       "  4.205752,\n",
       "  4.1958714,\n",
       "  4.180777,\n",
       "  4.176884,\n",
       "  4.179717,\n",
       "  4.1756716,\n",
       "  4.186036,\n",
       "  4.1780953,\n",
       "  4.180181,\n",
       "  4.1730504,\n",
       "  4.171473,\n",
       "  4.1738334,\n",
       "  4.1639833,\n",
       "  4.154662,\n",
       "  4.146443,\n",
       "  4.146165,\n",
       "  4.142267,\n",
       "  4.142902,\n",
       "  4.1295094,\n",
       "  4.1229115,\n",
       "  4.1232905,\n",
       "  4.1239963,\n",
       "  4.118799,\n",
       "  4.118685,\n",
       "  4.1207676,\n",
       "  4.1217036,\n",
       "  4.121556,\n",
       "  4.1220613,\n",
       "  4.113785,\n",
       "  4.1244617,\n",
       "  4.1224647,\n",
       "  4.113756,\n",
       "  4.1132717,\n",
       "  4.1098924,\n",
       "  4.1060553,\n",
       "  4.1040254,\n",
       "  4.102885,\n",
       "  4.1042166,\n",
       "  4.102471,\n",
       "  3.8400981,\n",
       "  3.7095323,\n",
       "  3.5731258,\n",
       "  3.4833543,\n",
       "  3.584402,\n",
       "  3.641061,\n",
       "  3.6805854,\n",
       "  3.7072096,\n",
       "  3.6606898,\n",
       "  3.6624706,\n",
       "  3.6848755,\n",
       "  3.7035408,\n",
       "  3.7447379,\n",
       "  3.7574973,\n",
       "  3.7765825,\n",
       "  3.7691512,\n",
       "  3.7697647,\n",
       "  3.7659178,\n",
       "  3.7922359,\n",
       "  3.820362,\n",
       "  3.8423953,\n",
       "  3.8425643,\n",
       "  3.834681,\n",
       "  3.8396683,\n",
       "  3.8429406,\n",
       "  3.8248065,\n",
       "  3.8098857,\n",
       "  3.820902,\n",
       "  3.8255177,\n",
       "  3.8122544,\n",
       "  3.8088338,\n",
       "  3.8291512,\n",
       "  3.831479,\n",
       "  3.8341548,\n",
       "  3.8352635,\n",
       "  3.8530655,\n",
       "  3.858108,\n",
       "  3.8646626,\n",
       "  3.8748279,\n",
       "  3.8876987,\n",
       "  3.8962305,\n",
       "  3.8991783,\n",
       "  3.90301,\n",
       "  3.889641,\n",
       "  3.8935697,\n",
       "  3.891338,\n",
       "  3.8980424,\n",
       "  3.8957279,\n",
       "  3.9062965,\n",
       "  3.908258,\n",
       "  3.9146602,\n",
       "  3.90544,\n",
       "  3.8997808,\n",
       "  3.8980238,\n",
       "  3.9044151,\n",
       "  3.9009366,\n",
       "  3.9056084,\n",
       "  3.9039114,\n",
       "  3.900964,\n",
       "  3.9009337,\n",
       "  3.9005039,\n",
       "  3.8987255,\n",
       "  3.90949,\n",
       "  3.9055014,\n",
       "  3.9083655,\n",
       "  3.9071507,\n",
       "  3.9061255,\n",
       "  3.9077952,\n",
       "  3.910345,\n",
       "  3.9141414,\n",
       "  3.9107897,\n",
       "  3.9099455,\n",
       "  3.9092536,\n",
       "  3.911625,\n",
       "  3.9082482,\n",
       "  3.909568,\n",
       "  3.910294,\n",
       "  3.908166,\n",
       "  3.9001315,\n",
       "  4.1642957,\n",
       "  4.1552467,\n",
       "  3.9785748,\n",
       "  4.0139527,\n",
       "  3.939361,\n",
       "  3.9458685,\n",
       "  3.957648,\n",
       "  3.9341958,\n",
       "  3.897552,\n",
       "  3.9392853,\n",
       "  3.8970916,\n",
       "  3.8834848,\n",
       "  3.909365,\n",
       "  3.892716,\n",
       "  3.8790305,\n",
       "  3.889782,\n",
       "  3.8714168,\n",
       "  3.8604774,\n",
       "  3.838858,\n",
       "  3.84283,\n",
       "  3.836572,\n",
       "  3.8311906,\n",
       "  3.8421357,\n",
       "  3.8421123,\n",
       "  3.84931,\n",
       "  3.8454225,\n",
       "  3.8382864,\n",
       "  3.8280237,\n",
       "  3.8265862,\n",
       "  3.8310344,\n",
       "  3.8292742,\n",
       "  3.812964,\n",
       "  3.8080401,\n",
       "  3.8105164,\n",
       "  3.81038,\n",
       "  3.7908506,\n",
       "  3.7777846,\n",
       "  3.77149,\n",
       "  3.7792213,\n",
       "  3.779348,\n",
       "  3.7909386,\n",
       "  3.7964172,\n",
       "  3.8066065,\n",
       "  3.8030577,\n",
       "  3.7964833,\n",
       "  3.7922873,\n",
       "  3.7904422,\n",
       "  3.7920983,\n",
       "  3.7815661,\n",
       "  3.782858,\n",
       "  3.7899323,\n",
       "  3.7876015,\n",
       "  3.788988,\n",
       "  3.78508,\n",
       "  3.7839115,\n",
       "  3.7905133,\n",
       "  3.7993126,\n",
       "  3.7993324,\n",
       "  3.799582,\n",
       "  3.8070824,\n",
       "  3.8045406,\n",
       "  3.8054388,\n",
       "  3.804227,\n",
       "  3.80902,\n",
       "  3.8054411,\n",
       "  3.8049126,\n",
       "  3.8061569,\n",
       "  3.7980382,\n",
       "  3.8016005,\n",
       "  3.805191,\n",
       "  3.8008792,\n",
       "  3.8010397,\n",
       "  3.8073332,\n",
       "  3.8075035,\n",
       "  3.8047464,\n",
       "  3.8000166,\n",
       "  3.7993417,\n",
       "  3.8014314,\n",
       "  3.791124,\n",
       "  3.9952981,\n",
       "  3.7365985,\n",
       "  3.7472365,\n",
       "  3.7945971,\n",
       "  3.7550976,\n",
       "  3.7595003,\n",
       "  3.7674563,\n",
       "  3.7764862,\n",
       "  3.7906473,\n",
       "  3.7878299,\n",
       "  3.7905838,\n",
       "  3.7680054,\n",
       "  3.7671804,\n",
       "  3.774772,\n",
       "  3.7802155,\n",
       "  3.7821689,\n",
       "  3.8127742,\n",
       "  3.829974,\n",
       "  3.84507,\n",
       "  3.8488917,\n",
       "  3.8461041,\n",
       "  3.848005,\n",
       "  3.860124,\n",
       "  3.8487587,\n",
       "  3.8442078,\n",
       "  3.850857,\n",
       "  3.8496718,\n",
       "  3.8469086,\n",
       "  3.8384614,\n",
       "  3.8269148,\n",
       "  3.8181026,\n",
       "  3.8046799,\n",
       "  3.7943928,\n",
       "  3.7902334,\n",
       "  3.796936,\n",
       "  3.788648,\n",
       "  3.7959356,\n",
       "  3.797686,\n",
       "  3.793846,\n",
       "  3.8040366,\n",
       "  3.8054771,\n",
       "  3.8250422,\n",
       "  3.8300362,\n",
       "  3.829486,\n",
       "  3.8363051,\n",
       "  3.8321373,\n",
       "  3.8427455,\n",
       "  3.8408368,\n",
       "  3.837961,\n",
       "  3.851069,\n",
       "  3.8478737,\n",
       "  3.8441722,\n",
       "  3.8521144,\n",
       "  3.8533447,\n",
       "  3.8559442,\n",
       "  3.852411,\n",
       "  3.8471525,\n",
       "  3.8449938,\n",
       "  3.8491187,\n",
       "  3.849997,\n",
       "  3.8514922,\n",
       "  3.8471699,\n",
       "  3.842838,\n",
       "  3.844081,\n",
       "  3.8466735,\n",
       "  3.8413477,\n",
       "  3.8323255,\n",
       "  3.8362179,\n",
       "  3.8370101,\n",
       "  3.8387172,\n",
       "  3.8308308,\n",
       "  3.8376431,\n",
       "  3.834978,\n",
       "  3.8256288,\n",
       "  3.8216698,\n",
       "  3.8174279,\n",
       "  3.8205914,\n",
       "  3.8116114,\n",
       "  3.8234496,\n",
       "  3.9885674,\n",
       "  3.8158474,\n",
       "  3.845254,\n",
       "  3.8314505,\n",
       "  3.807949,\n",
       "  3.711634,\n",
       "  3.7590954,\n",
       "  3.7977533,\n",
       "  3.782681,\n",
       "  3.774908,\n",
       "  3.8070366,\n",
       "  3.7831147,\n",
       "  3.777626,\n",
       "  3.7580478,\n",
       "  3.7614298,\n",
       "  3.758882,\n",
       "  3.7577448,\n",
       "  3.7314177,\n",
       "  3.73502,\n",
       "  3.737449,\n",
       "  3.7237437,\n",
       "  3.7092116,\n",
       "  3.6975951,\n",
       "  3.715144,\n",
       "  3.739013,\n",
       "  3.7411337,\n",
       "  3.7568219,\n",
       "  3.756944,\n",
       "  3.7515407,\n",
       "  3.7609153,\n",
       "  3.7662928,\n",
       "  3.7697072,\n",
       "  3.7952752,\n",
       "  3.7861776,\n",
       "  3.7748055,\n",
       "  3.7672,\n",
       "  3.760607,\n",
       "  3.7570226,\n",
       "  3.7569685,\n",
       "  3.7618308,\n",
       "  3.7624347,\n",
       "  3.7605724,\n",
       "  3.7602487,\n",
       "  3.7557847,\n",
       "  3.7678783,\n",
       "  3.7609806,\n",
       "  3.7631218,\n",
       "  3.7634718,\n",
       "  3.763374,\n",
       "  3.7622046,\n",
       "  3.7618454,\n",
       "  3.7597184,\n",
       "  3.765576,\n",
       "  3.7699206,\n",
       "  3.761101,\n",
       "  3.7539177,\n",
       "  3.764494,\n",
       "  3.7647688,\n",
       "  3.7635171,\n",
       "  3.7614887,\n",
       "  3.7583113,\n",
       "  3.757825,\n",
       "  3.7545226,\n",
       "  3.7526684,\n",
       "  3.7521312,\n",
       "  3.746389,\n",
       "  3.7458344,\n",
       "  3.745933,\n",
       "  3.740379,\n",
       "  3.7435467,\n",
       "  3.7480502,\n",
       "  3.750188,\n",
       "  3.7513,\n",
       "  3.7495353,\n",
       "  3.7499394,\n",
       "  3.7522566,\n",
       "  3.7591133,\n",
       "  3.755808,\n",
       "  3.7445273,\n",
       "  4.042598,\n",
       "  3.9068484,\n",
       "  3.70672,\n",
       "  3.7651205,\n",
       "  3.738316,\n",
       "  3.7915955,\n",
       "  3.7902753,\n",
       "  3.7697887,\n",
       "  3.7784395,\n",
       "  3.7508273,\n",
       "  3.7243395,\n",
       "  3.7310455,\n",
       "  3.728146,\n",
       "  3.6970882,\n",
       "  3.6803563,\n",
       "  3.6958103,\n",
       "  3.705718,\n",
       "  3.6740215,\n",
       "  3.6799047,\n",
       "  3.6985335,\n",
       "  3.7212148,\n",
       "  3.7266195,\n",
       "  3.6958814,\n",
       "  3.7032,\n",
       "  3.7014391,\n",
       "  3.6985838,\n",
       "  3.6910803,\n",
       "  3.6881003,\n",
       "  3.6742933,\n",
       "  3.6737137,\n",
       "  3.6741786,\n",
       "  3.6743765,\n",
       "  3.67178,\n",
       "  3.6714191,\n",
       "  3.6555727,\n",
       "  3.645299,\n",
       "  3.6422024,\n",
       "  3.6392095,\n",
       "  3.6318457,\n",
       "  3.6286225,\n",
       "  3.6332638,\n",
       "  3.6318781,\n",
       "  3.631775,\n",
       "  3.6225653,\n",
       "  3.6178012,\n",
       "  3.6163177,\n",
       "  3.612312,\n",
       "  3.6033833,\n",
       "  3.598995,\n",
       "  3.6007693,\n",
       "  3.6068618,\n",
       "  3.6059253,\n",
       "  3.6144712,\n",
       "  3.6153502,\n",
       "  3.613887,\n",
       "  3.614672,\n",
       "  3.6162372,\n",
       "  3.616692,\n",
       "  3.6124957,\n",
       "  3.620357,\n",
       "  3.6250134,\n",
       "  3.6299267,\n",
       "  3.6278512,\n",
       "  3.627192,\n",
       "  3.628215,\n",
       "  3.635147,\n",
       "  3.6307125,\n",
       "  3.6338146,\n",
       "  3.6308641,\n",
       "  3.6278648,\n",
       "  3.6297634,\n",
       "  3.6345356,\n",
       "  3.6378913,\n",
       "  3.6396856,\n",
       "  3.637535,\n",
       "  3.639574,\n",
       "  3.6382475,\n",
       "  3.6376348,\n",
       "  3.6340213],\n",
       " 'mean_val_loss': [1.9939928, 1.8412073, 1.844857, 1.930764, 2.0103106],\n",
       " 'min_val_loss': 1.8412073,\n",
       " 'test_loss': [tensor(2.1238),\n",
       "  tensor(1.8693),\n",
       "  tensor(1.9509),\n",
       "  tensor(1.8589),\n",
       "  tensor(2.0245)],\n",
       " 'train_time': 593.6578595638275,\n",
       " 'trainargs': {'activation': 'ReLU',\n",
       "  'batch_size': 512,\n",
       "  'checkpoint': True,\n",
       "  'checkpoint_path': '/content/gdrive/My Drive/rxn_ebm/',\n",
       "  'dropout': 0.5,\n",
       "  'early_stop': True,\n",
       "  'epochs': 50,\n",
       "  'expt_name': '2048_1layer_1neg_rad2_ReLU',\n",
       "  'fp_radius': 2,\n",
       "  'fp_type': 'sep',\n",
       "  'hidden_sizes': [256],\n",
       "  'learning_rate': 1e-05,\n",
       "  'min_delta': 1e-05,\n",
       "  'model': 'FF_sep',\n",
       "  'model_seed': 1337,\n",
       "  'num_neg': 1,\n",
       "  'optimizer': torch.optim.adam.Adam,\n",
       "  'output_size': 1,\n",
       "  'path_to_pickle': '/content/clean_rxn_50k_nomap_noreagent.pickle',\n",
       "  'patience': 3,\n",
       "  'prodfp_size': 2048,\n",
       "  'random_seed': 0,\n",
       "  'rctfp_size': 2048}}"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "pvI5jZOcwEnw",
    "outputId": "d63f1b3c-dfc0-4a3f-e740-2812e3ec28bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:48<00:00,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_time: 7941.052913427353\n",
      "test_loss: [tensor(3.6462), tensor(4.0916), tensor(3.9566), tensor(3.9063), tensor(3.8079)]\n",
      "mean_test_loss: 3.8817272186279297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats = test(model, stats, trainargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DvDHRmfi3ZaF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [
    "twEVWkj63ZZ5"
   ],
   "name": "rad = 3 num_neg = 5 Feedforward_+_diffFP_.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
